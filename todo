* riccardo proj, calibdir reproduce, psffb jobs, large-gate deadtime, fairshare, rix practice, gem-gdet, andor 1Hz, github actions (curl vs. requests), high rate gmd/xgmd, andor psmon, psana1 live-mode stat workaround, mfx fibers, more timing cables, integrating andor (and step scans, goose also set per andor image, multiple andors with different integration times, 10ms-1s), epixM, epixUHR

mona proj:
- excluded detector list in confluence documentation
- eliminate filter_fn text from confluence documentation (now smd_callback)
- test live mode with full tmo.cnf
- more timing fibers for ric epixhr emulation
- dgramedit requires config_dgramedit argument. should it just be config_dgram?
- ttool background (work with ric)
- destination callback (conflicts with integrating detector?). work with silke,andy@txi
- epixM

rix andor:
- make it easier:
  o input ms instead of buckets (1buckets=14/13us)
    - readoutime/exposuretime/beamrate/nrepeat
  o create easier access to the xpm PV's (without PVAADDRLIST)
  o mike/kukhee eventcode xfave
- make it more real
  o for 5ms readout 4648 for number of buckets (using 14/13us per bucket)
- add event code filtering to the analysis script
- why didn't we see andor in ami with full image?

- wave8 deadtime, mono encoder crash, hsd deadtime @10kHz

integ andor issues:
- psmon rank0 with "mpirun -n 5 python andor.py" (maybe just call publish.init()?)
- matt's dropped-l1 counter
- no unaligned_andor_norm in h5?
- put srv_node in elog db
- wrapper scripts to watch for updated srv_node for psplot
- do normalization correctly if andor shot is dropped due to deadtime
- understand batch_size=1 param to datasource

s3df issues:
- on/off shift: qos on the account gets set automatically
- weka core pinning
- fairshare?
- ampere preemptable
- srun pmix "canary"
- open ports
- live mode (nfs and native?)
- multicluster

tmo detectors:

Detectors to be ready in June: 2X FIMs (2 on KB1), ATM opal&piranha for IP1, FZP opal&piranha, HSDs, BLD, Laser wav8, IM5K4 

Detectors to be added by the end of calendar year 2023: two more FIMs on KB2 and IM6K4, ATM opal&piranha for IP2, another Laser wav8 for IP2


- root replacement for phil
- epixM
- fakecam epixhr (big project, using valerio's drp-python)

omar: ipmi, daq-tmo-hsd-02 .ko file

xpm0: event code 272,273 33kHz 35kHz

- global: timing glitches, move atca to mon001 (and permanent netconfig), running neh-base.cnf from 4.5.17, piranha background (jan.) (slowupdate and L1)
- tmo: hsd's, peppex hsd's/opal, acr feedback test
- rix: no ami grafana
- lower priority: fibers to clean up

high rate commissioning:
- check l0delays and round-trip times and buffering

srcf to do:
- atm edge-finding for piranha and feedback
- daq kafka bus all-files-started
- protect against hsd firmware unhappy if too many fex peaks (matt)
* psana damage handling
- remember detector readout groups in cnf? (done by caf)
- epicsarch not writing out 1MHz l1accepts (or goes in slow readout group)
- xpm fanouts in srcf (maybe not urgent since only piranha's need them?)
*- auto coupled deadtime xpm register support in timing system python
- time in detectors
*- need slowupdate to obey deadtime? (messes up Ric's control-Z test of deadtime in srcf)
*- move all tmo devices to xpm:2
- move the pyxpm processes/network to SRCF
  - could also have local connections from e.g. rix nodes to the rix atca crate
  - short term: DAQ:NEH:XPM/DAQ:TMO:HSD/RIX:FIM:W8/MR4K2:FIM:W8/MR3K4:FIM:W8/MR2K4:FIM:W8
    goes in the gateway RW
- ami-settable data reduction params (e.g. thresholds for hsd)
*- scale to many drp's
- high rate dets
  - piranha
*  - bld
  - hsd
  - wave8
  - epixhr
- low rate det
  - epicsarch
  - pvadet
- roentdek alg (done by mona)
- chunking (done by caf)
*- python drp
- rixccd
- psana
  - live-mode
*- dynamic resource mgmt
*- automatic off by one detection

detector requests:
- (det) rixccd archon (installed in hutch march 2023, beam oct 2023?)
- superconducting BLD (fall 2022)
- (tid) high-rate mono-encoder
  o interpolated absolute encoder (march 2023)
  o high rate relative encoder (december 2023)
- dream hsd's in jan. 2023
- (unapproved) ikon (for SVLS, kristjan et al., early 2023, perhaps princeton mid-late 2023?)
- (tid, det?) k-microscope dld (test early summer 2022, production in early 2023?, beam summer 2023)
- ami roiarch with 3D (mikhail)
- (det) uxi needs more work from dan (lcls1)
- (standalone approved) (tid, det) tixel (tmo standalone rogue tmo, dec. 2022?, daq integration if looks good)
  o bojan in TID is doing the work
- (tid, det) epixM (integration prototype (no asic, no emulation) from tid beginning of june 2023? version with asic end of june?, 2 320kpx, 5kHz, commission april/june 2023, probably delayed),
  - no fixed gain mode, only one autorange mode AHL, some gain ambiguity that needs to be resolved
  - integrate in november 2022.  full cam in june 2023.
  - MTP24 broken out to 3 MTP8 fiber 6.4GB/s
  - 384*192*2 per camera (737MB/s per camera @5kHz)
- (tid,det) epixHR (prototype quad in march, start work sept 2022, 2Mpx, 5kHz, summer 2023) single MTP24 broken out to 3 MTP8 fibers: prototype in august 2022, beam in sept 2022 (and dec 22). quad in November 2022.  full cam in march 2023.  full 2M det commissioning june 2023.  quad is different than single.
  - 5 to 10 fibers pairs needs to be converted (use local xpm to generate
    multimode).  maybe 12 if edge-ml stuff.
  - 2 small epixHR's (140kpx?) single MTP12
- cookiebox (early 2024): guarantee 8 channels, hope for 16
- (will approve) (tid) opal replacement (s991? photometrics kinetix? giacomo thinks perhaps alvium?) https://www.alliedvision.com/en/products/alvium-configurator/alvium-1800-u/240/#_configurator
- (approved for standalone) (tid,det) timepix for rix: standalone mode in July 2022 (256x256 px,
  o get start-time and time-over-threshold, ~2kHz, could be faster)
  o working with anton tremsin at berkeley
- (unapproved) (det) varex xrd 4343 cameras for mec? (offered assistance, lcls1, needed april/may 2023)
- (unapproved) orca quest hamamatsu cxp camera? (lcls1, xpp? matt seaberg takahiro)
  https://www.hamamatsu.com/eu/en/product/cameras/qcmos-cameras/C15550-20UP.html
- (tid, det) small epixUHR (200x200): (emulator available end of feb. 2023) beam-test with daq april 2023 (35kHz).
  not identical to epixHR.
  - daq integration with firmware emulator in jan 2023?  prototype 1 asic in april 2023? beam time soon after
- andor iXon for UED (flexible, June/July 2023?)

* saxs/waxs data reduction (chuck)
* fluctuation saxs/waxs data reduction (chuck)
* manage running-condition dependent calibration constants (e.g. rate dependent) (tweak serial id?) (mikhail)
* improve destination callback (mona)
* monitoring new serial numbers for LCLS2 (mikhail)
* release DMA buffers earlier? currently released at same time as pebble (ric)
* small VDS "link files" seem to have absolute path to partN files. should be relative path.
* swapping slurm jobs script (ro)
* multi camlink cameras in a node
* configuration management/control for calibration/fexA/fexB
* individual deadtime sources in grafana so we don't need to run xpmpva (matt)
* test timbvd reduction for txi and libpressio
* update aes-stream-driver to 64-bit so we can unpin rogue
* eliminate intermediate "panel" files on disk and put in database (for epix10k/jungfrau) (mikhail)
* calibration constant provenance (mikhail)
* h5 full-pathname issue in part0 files
* ami python editor returning array with one element into ScalarPlot generates no visible error (but see error in logs)
* understand/fix "384" intermittent libfabric ib completion queue issue (ric) (currently "fixed" by running eblf_pingpong)
* dgramedit requires config_dgramedit argument. should it just be config_dgram? (mona)
* long eb connect times in rix (ric)
* more hsd's for simultaneous cookiebox/dream running (14+16 channels)
  but maybe not more hsd's if tixel works (weaver)
* not all errors show up in ami gui, e.g. missing attribute error (ddamiani)
* andor pedestal subtraction (mikhail)
* managing sharing: readout groups, drp nodes, cnf files, timing system connections
 - Chris Ford is working on a shared-DAQ resource allocation manager
 - Matt Weaver has this big-optical-switch ("BOS") which is principle allows more flexible use of the shared DAQ nodes
 - right now we have rix/tmo "official" setups.  we should probably create another one for acr, and then another one for one-off setups (like what Bill Schlotter is doing tomorrow).
 - make drp nodes more uniform?
* mask editor (mikhail)
* when running ami in daq detector list is intermittently not updated (ddamiani)
* have daq bluesky scan scripts default to "scan" as scan detector name to avoid psana crash (caf)
* ability for select-gui to allow teb's to be deselected (down to 1) (mikhail)
* epix100/epixhr deadtime not working (10Hz with epix100)
* ami angular integration
* per stream and per run event/dmg counts (caf)
* better management of drp .service files (shared drive, puppet, kubernetes?)
** integrating detectors (daq and psana)
* configure andor with configdb?
* silver behenate fitting (including z-stagger) for panel positions (mikhail)
** benchmark SZ and other reduction for real expt data (mikhail)
  - can SZ handle gain-range detectors?
* make it harder to leave out timing system
* algorithm for non-quantized det-image charge sharing (mikhail)
* raise exception instead of crashing when idx/smd files are missing in psana1
* broadcasting non-epics slowupdate data (e.g. ttool bkdg)
* create an example of parallel smallh5 with mpi/dask (riccardo)
* test psana2 live mode (mona)
* move encoder to tcp (caf)
* crystfel to psana geom (mikhail)
* ami mypy daemon is long-lived: can cross release boundaries
* configdb management for changing fex/teb decisions
* run lcls1 jungfrau at 120Hz (ddamiani, dan)
* move TMO to xpm:2 (like RIX)
* reduction algorithms
* turning low rate data into per-shot high rate (e.g. ttool background, encoder) ami+psana (mona)
* timing system register to count inhibited l1accepts for integrating detectors (matt)
* timetool slowupdate background handling in psana (mona)
* elegant early shutdown of psana with smd0 irecv for mtip? (mona)
* ami2 array thresholding (ro)
* procmgr performance with hundreds of nodes (caf)
* explore object stores for psana like ceph (mona)
* alias not in xtc file so more flexible
* psana datasource for drp (valerio/mona)
* teb having memory allocation problem after too many deallocate's (ric)
* select monitoring destination for particular events (ric)
* avoid missing 1 second of epicsarch data at beginning of run
* small daq teststand for automated continuous-integration tests?
* multisegment epicsinfo in pvadetector
* ami controlling DRP general ROI with masks via configdb? (mikhail)
* twocanary (toucan) in srcf
* code/results migration to/from nersc/sdf
* psmon only update one of multiplot (ddamiani)
* psana test to verify that all detector interfaces follow the rules
* psmon XYPlot format list of one item breaks multiple lines silently (ddamiani)
* more psmon examples for andy (ddamiani)
* consider adding "safe" Xtc::alloc with max-size argument to avoid fixed-buf overruns (or maybe in higher layer)
* off-by-one enabling automation (stoppers)
* xface for ami controlling detector params
* web display of ami plots
* mhz bld (matt)
* uniform (epics?) interface to detector config registers for config scans
* automatically push releases for lcls1/lcls2 to nersc/sdf/other
* makepeds/calibrun regression tests and simplification or freeze? (mikhail)
* opals for different daq's on same machine (larry+pcds)
* fix psana1 test release LD_PRELOAD hack caused by removal of
  ' -Wl,--copy-dt-needed-entries -Wl,--enable-new-dtags'
  in SConsTools/psdm_cplusplus.py for gcc48.
* psana dbase access for live mode list of files
* handle bad external timing better? (matt)
* deadtime-per-detector in grafana
* generic detector calibration constants (e.g. manta)
* teststand: kcu's to acc nodes (01,02,05,06), bring back ffb, fix ib manager and moving it to switch
* make swmr work with small-h5
** for phil: configdb_readxtc and compare configs (ro, caf)
* archon for rix (dan)
* improved psana1 tests (ro)
* lcls2 common-mode for panels in different gain ranges (mikhail)
* plims for lcls2 (mikhail)
* mpi jobs not exiting if one core crashes
* managing prometheus files in promdir
* put procstat in prometheus (or hard to duplicate all functionality?)
* how to deal with pickle in the calibdb? (mikhail)
* epicsarch support for strings (ric says it's hard)
* support damage in psana, including counting (mona)
* protect against use of keywords as epics aliases, consider using namespacing. write a sanitizer for epicsarch files (use "import keywords"?)
* send clearreadout to pvadetectors
* support uniform 3D arrays in areadetector interface
* remove git passwords for relmanage
* eliminate opal lcls1->lcls2 timing toggling (larry, ben)
* multiseg epics (kwargs or collection segids?) (mona)
* jungfrau/epix dark shot pedestals
* psana1: implement idx using smd
* correct setting for hyperthreading? (ric says important for MHz)
* psana2 idx mode
* psana1: add epix firmware id in epix id's for phil
* upgrade jupyterhub or use on-demand?
* general test that all public detector attributes are method taking event with return type (ro)
** support multiple cameras on one drp node
** one exe writing multiple files to improve performance
** xtcdata duplicate names throw
* move away from afs (mv pdsdata/psalg repo)
* spares
** make everything work with Debug instead of RelWithDebInfo (valerio, ric)
* move readout group config info from segment levels to ts
* optionally don't record selected detectors to disk
* mikhail: roentdek with quadanode
* fix camlink converter box with opal where only one strip works (requires camlink powercycle)
* daq window placement
* parallelized calibrations (mikhail)
* state machine confusing (ric email) lower priority?
* monitor deadtime per-lane (matt)
* improved grafana (like ganglia)
* move other hutches to lcls2
* allocation of readout groups and devices, lanes
** shared drp node allocation mechanism (caf)
* LCLS2 drop shots (bykiks evtcode 161)
* MHz with real tmo analysis including pre/post processing partitioning (mona)
* create more platforms for procmgr, perhaps with offsets for each hutch based on xpm? (caf)
* procmgr don't complain about undefined platform when all ports are specified (e.g. neh-base.cnf)
* practice power outage
* parallel jupyterhub with visualization (copy euxfel?) (riccardo/mona/wilko/valerio)
* portable gpu detector corrections (kokkos/hip/openmp/opencl?)
* deploy releases everywhere, containers? (valerio) (nersc, sdf, new/old psana1)
* unified/integrated timetool calibration
* fix failing new-style psana1 tests (valerio)
* shifter mounting permissions (mona, johannes)
* send multiple copy of events to shmem like lcls1
** small h5 ebeam/gasdet automatic storage (ro)
* small h5 ragged arrays
* meb (or later layer) broadcasts events to all clients (ami, python) (ric)
* read-while-write smallh5
* test daq sequences (caf)
* psana1 MTRX:V2 geometry (mikhail)
* update procServ (caf)
* slow updates need to obey deadtime (matt)
* setting msgdelay in timing system? (matt)
* syslog print throttling (caf)
* off-by-one (psana1/2) (ro)
* off-by-one support for low-rate devices
* units support in det xface
* put psana2 (and psana1?) on conda-forge
* calibdb dns issue (in travis macos build)
* in psana SRV callback only persist some fields to h5?
* once we have real data, work more on timetool calc in firmware
* dlopen for reduction alg
* scalable calib-fetch solution for shmem/drp
* sdf/nersc calib-dir sync (wilko)
* psana1: continuous integration of py3 (jenkins)
* lcls2 configdb tools: history, delete (into "trash" folder for recovery?) (ro)
* algorithms (drp/ana, e.g. beam-center finding)
* singularity at slac
* support more python versions
* S3DF support including ARP
* automatic users add to sdf lcls queue (wilko, valerio)
* python DRP? (valerio, ric)
* fiber power readings from timing system kcu's (matt)
* peaknet
* ami:
  - josh: nanosecond xpcs: 2 pulse acqiris.  ratio of peak areas, correlation
    export results to ACR
  - josh: xpcs photon counting, working with chuck: accumulate statistics
    and then fit, number of photons per shot in histogram (talk to chuck
    and silke sxrm23). nicholas burdet (shared postdoc)
* timetool (ben): tag to front end, fiber power, toggle xpmini->lcls2 timing, clear readout
* simpler interface for controlling teb/meb?
* xtcav daq recording epics variables as well (and also bld values?)
* xtcav/psocake py3 compatible

**********************************************************************

1 rack of computing for ffb
 - 1000 cores 8 nodes in 4u
ffb from 0.5PB to 1 or 2
sdf storage 6 jbods 1.5PB in 1 jbod in 4u for 50k + $overhead
enlarge sdf compute ?
switching to new srcf-sdf in april

**********************************************************************

2 filesystems: ffb (700TB) and offline (12PB)
with bigger disks can move ffb from 700TB to 1.4PB
700TB = 10 hours at 20GB/s

gpu:
2milan 32 high clock rate
4 nvidia a100

1 box 4 servers 2x64 core 1 box 512 cores 5K cores total

**********************************************************************

peppex cabling:
first cassette count from 1;
slot 2 hsd timing
slot 3 hsd leftmost
slot 4 hsd second from left

second cassette count from 1:
slot 4 hsd second from right
slot 5 hsd right
slot 6 opal

hsd timing plugged into xpm4 amc0 port 3 counting from 0

**********************************************************************

timepix discussion 06/06/22:

can use external trigger or free-running: we will use EVR-generated 120Hz trigger
main physics question to answer: can we separate one photon from two
will run at 120Hz
to synchronize with DAQ data can send extra ttl reset on a "slow" DAQ sequence to reset FPGA counter: this is more complex so don't do this
BNC or SMA with 50ohms
timing-in is easy: can use long exposure times and measure the time of the signal
jyoti will set up remote desktop to access anton's windows machine in the hutch via ICS

integrating comm protocol:
- timepix3 is event-driver (current one is timepix1)
- integrate timepix3
- use their fpga
- send jumbo udp packets on fiber. each photon is 6 bytes.
- suggests using their fpga timestamp

**********************************************************************

4000*2300*2*120=2GB/s (100Gbit enet)
triggering over cxp
fiber or bnc readout? (quad cxp6)
many other detectors (8 definite, 2 possible, varex)
lcls2: front-end box with timing and quad cxp6 in, pgp out
software timestamping for lcls1?  lcls2: firmware

**********************************************************************

rename roiarch?
masking and normalization (e.g. epix gaps)
arch outside image

- masks (define in ami?)
- 2d, 3d image
- q option with more parameters
- inherited/coupled parameters for multiple roiarch's

broken ami: pop, xtcav, hsd, photfind and linearity, xpp
good ami: blobfinder, hitfinder, flyscan, wf sideband sub, stepscan, wf integration vs delay stage, normalization of andor, ratio of peak areas and epics, calculator, complex, timestamp plot, export, reference plot

phil recommends divide by 16adu/kev
mikhails formula (sig-ped)/gain to get kev
det.gain returns something to divide or multiply by?
- silke multiplies by det.gain for epix100
epix10k 10 for high gain .01 for low gain
phil uploads adu/kev because mikhail divides
may be inconsistent with documentation
consistent handling (adu/kev or kev/adu) of:
- uploaded numbers
- file on disk deployed by makepeds
- value by det.gain
- documentation
epix100, rayonix for lcls1.  try to be consistent in lcls2
gain goes into pixel_gain file
mikhail has 0.06kev/adu

other detectors (jungfrau, epix10k): adu/kev->divide
epix100: ev/adu (.06kev/adu)->multiply (lcls1, other way round for lcls2)
many files have relative gains (around 1)
propose lcls1 "gain-factor": kev/adu

mark hunter:
institutional/personal pressure (sebastien helped prioritize)
collaborative vs. competitive access to resources

**********************************************************************

ami-local -b 1 -f interval=1 psana://exp=ueddaq02,run=569,dir=/cds/data/psdm/prj/public01/xtc/

dan and seshu
scripts vs ami
people develop their own libraries (personal/hutch):
  commonly used pieces moved into core
three categories:
  using existing boxes
  creating custom boxes
  detector guts
mask gain bits
thresholded image
managing libraries (avoiding creeping featurism)
editing library files with text editor
accessing calibconst: raw-pedestal (psana/psana/detector/UtilsEpix10ka.py
customizing epix calibration
controlling kwargs, e.g. cmpars
https://confluence.slac.stanford.edu/display/PSDMInternal/Detector+Interface+Proposal

**********************************************************************

rix high-rate risks:

moving DAQ to SRCF:
ric claus work
- high rates: hsd, wave8, piranha, bld
- selecting events in ami
matt: high rate bld
switching to LCLS2 timing
dan archon
s3df 

risks:
complexity of running detectors at different rates
high-rate mono-encoder (two?), high rate bld
using sequencer so andor can cleanly integrate over shots
new dets: epixM, high rate mono, archon, bld
practicing high-rate rix analysis in advance
managing which events AMI sees
s3df delays
IOC detectors (like andor) need new module from Kukhee

**********************************************************************

tmo detectors:

2 piranhas (fzp, atm) + 1 opal (fzp) + 1 opal (cvmi)
1 for dream atm + 1 spare
currently have 4 (2 cvmi, 1 atm, 1 fzp)
bld

rix priority:

2 andor newton
3 wave8
1 atm piranha
1 atm opal
hsd's?
bld
high-rate mono encoder and/or step-scans

**********************************************************************

general to do:
- recabling to srcf
  o use data volume to each node to figure out cabling
  o (optional) multi-camlink operation.
    - could program timing link on reboot
    - every process could set full link mask?
  o what should go through the bos?
- switching to LCLS2 timing (Dec. 1?)
- 33kHz BLD: ebeam, gmd, test asap. waiting for software development.
- commission s3df (but keep old system going too)
  o psana
  o smalldata
  o movers
  o mon-node calibdir
- spares (piranha in particular)
- migrate ctl nodes (move hsd processes, but not mono encoder)
- move timing crate from fee alcove to 208
- xtcav "nice" for acr?
- live mode


tmo to-do:
- when can we switch from opals to piranhas?
- (optional) multi-camlink running
- will fzp/atm opals run at same time?
- hsd fex overflow damage flagged?
- test hsd prescale raw waveform buffering
- move away from xpm0 to xpm2 for all tmo timing

tmo schedule:

[cvmi_opal_1, oct 27, nov 19] - retire date is negotiable, (maybe needs to be earlier?)
[cvmi_opal_2, oct 27, nov 19] - retire date is negotiable, (maybe needs to be earlier?)
[atm_opal,       oct 27, possibly run 22]
[fzp_opal,        oct 27, possibly run 22]
[atm_pirhana, oct 27, not planned]
[fzp_pirhana,  oct 27, not planned]

rix to-do:
- integrating-andor timestamping (I think Dan said this is fine as-is)
- accelerator burst-mode operation
- scans with burst-mode (one burst is one andor event)
- (somewhat optional) high-rate interpolated mono encoder for fly-scans
  o offline second-pass with interpolation could also work
  o step-scans with slow mono are another option

**********************************************************************

100kHz data volumes:

- wave8: 8waveforms*256samples/sample*2bytes/sample*0.1MHz = 400MB/s
  need to add FEX data too
- piranha: 2048px*2bytes/px*0.1MHz=410MB/s
- hsd full wf: 60000samples*2bytes/sample*0.1MHz = 12GB/s
  max waveform length appears to be ~60000 samples looking at xtc2 file?
  FEX needs to reduce it to 4GB/s (3x)
- bld: 336bytes/event*0.1MHz = 33MB/s
  (from xtcreader -f /cds/data/psdm/prj/public01/xtc/tmoc00118-r0222-s008-c000.xtc2)
- timing: 309bytes/event*0.1MHz = 31MB/s

recabling proposal:
- tmo timing (including pvadet, bld)
- rix timing
- (4) tmo camlink nodes
- (2) rix camlink nodes
- tmo/rix fim node (5 fims)
- (2) rix hsd nodes
- (7) tmo hsd nodes
- peppex hsd node
- (2) tmo/rix ami
- (2) tmo/rix teb
- (2) tmo/rix mebuser

total of 25 nodes
5 spares

**********************************************************************

3.10 errors:

qt errors in groupca/xpmpva
control.py qt setvalue wants int but getting float
piranha rogue errors

**********************************************************************

rix analysis

point detector APD via wave8, background subtract
direct andor integrate over shots and background subtract
vls andor no background
might want to run at 1Hz with bigger images
filtering (using the signal itself like APD, or other det: I0 from fim's, GMD, use goose trigger or bykik (laser/xray on/off))
binning in terms of delay (vitara) eventually use timetool
fly-scan with mono-encoder and bin
after binning (vitara, eventually timetool, mono encoder) do normalization (I0 kb fim's)
no peak-finders
could run droplet on vls andor (droplets may merge) background low enough that not necessary?
could we run the andors in different modes?  might be complex in burst mode

march: beam at the mono
april: beam in chemrix

mockup piranha at 33kHz by modifying opal?

propose 33kHz analysis as "worst case" (gets easier with bursts and integrating detectors)

**********************************************************************

according to bos:
cmp11 goes to amc0 port 5 (checked)
cmp12 goes to amc0 port 3 (checked)
cmp06 goes to amc0 port 6 (trusting bos aliases)

**********************************************************************

worries:
political assigning time
off-shift handling?  preemption or queue position?
reservations more wasteful
preemption latency

1 - reservations + head-of-queue-qos
2 - ffb preemption qos + head-of-queue-off-shift qos
3 - 4 layers of preemption

association: priorities based on partition/account/reservation/qos (either for preemption or position-in-queue)
slurmacctmgr command is used to manage priorities

**********************************************************************

outer product options:

need two: one with hsd channel itself, and one with piranha readout

- only do half of the symmetric outer product (with hsd channel itself)
- reduce the sample rate by ~4
- reduce window to 2us (12000 samples) 1us @1MHz
- use peaks (guess badness is 5+-3)
- replace with tixel eventually (which does thresholding)
  o may not use tixel because of the thresholding, depending on expt
- use pre-scaled full-waveform events for bkgd?

**********************************************************************

epixhr

include forced-low, forced-medium
for AHL, AML
new format for timing (2 lanes each with 2 asics, each with timing info)

saci bus lockup?
same firmware on receiving end?

**********************************************************************

txi hxr kpp measurements:
- postponed from april 29? may? june?
- 1kHz and 10kHz SC beam going through hxr undulators into txi
- wave8 daq (synced trigger)
- want 100Hz old hxr gasdet bld, gate width up to andy/philheimann
  o will do with unsynchronized epics.  silke says someone is working
    on this (called "GEM")
- NO ebeam bld (hxr bld (e.g. gasdet, some ebeam)  will be 120Hz?)
- epics
- no daq camera. use untimestamped controls camera, perhaps recorded in daq.
- need single mode fibers, may need cassettes
- timing in wave8 will be done earlier in tmo
- just wave8 and a bunch of epics variables
- other epics: power meter (andy, can get name from jyoti) controls uv sensitive diodes
  (alyssa, jyoti can provide epics names).  get gem-gdet names from silke (she
  said it might be working on may 11, 2023)

tickets:

https://jira.slac.stanford.edu/browse/LCLSECSD-1546
https://jira.slac.stanford.edu/browse/LCLSECSD-1874

**********************************************************************

rix fim 0,1 ioc's down
andor_dir ioc down
andors/manta need to move to lcls2 timing
no bld

**********************************************************************

xpp epixhr test:

xpp rack 2 elevation 22-25 ph5 (labelled on right): 4 free pairs on left?
xpp rack B950S-30 elevation 46 cassette labelled slot 3
strand 3,4 flipped in rightmost xpp cassette in 208, 1,2,5,6,7,8 good
leftmost xpp cassette in 208 strands 7,8 go to xpp hutch slot 1, fibers 7,8 (currently has something plugged in in the xpp hutch, but not in 208

- plug epixhr "data1" (fiber 1 on breakout cable, counting from 1)
  into rack 30 slot 3 fibers 5,6
- plug epixhr "data2" (fiber 2 on breakout cable, counting from 1)
  into rack 30 slot 3 fibers 7,8
- plug epixhr "timing" (fiber 4 on breakout cable, counting from 1)
  into rack 30 slot 3 fibers 1,2

in room 208 use top xpm right-most amc (0, I think) port 4 (counting from 0)
data1 goes into bos 1.6.7->5.2.5
data2 goes into bos 1.6.8->5.2.6

txi 3 cassettes free
txi rack b950s-13 elevation 43

for the xpp wave8 data silke writes:

XppSb2BeamMon	
239.255.24.75	XPP-SB2-BMMON	XPP Local

need to add second bld device with "ipimb" structure with new
multicast group
239.255.24.40
https://confluence.slac.stanford.edu/display/PCDS/Notes+on+Photon-Side+BLD+Generation
https://confluence.slac.stanford.edu/pages/viewpage.action?pageId=138790412

There also is SB3 (...76), but I think that is the device that is current an LCLS2 wave8. But check both, one should work, otherwise inquire with Vincent

**********************************************************************

conversation with xinxin

this problem
crashes
slowness
pyeditor not saving (inputs and outputs disappearing)

align pinhole and gas-cell
normalize with wave8
sum thresholded/normalized image to measure background (time-window or infinity)
plot summed number vs. x/y/tilt/roll for pinhole and gas-cell

use mean vs. scan

scatter sum vs. motor

want uniform background, compare quadrants (make difference close to zero)

ami-local -b 1 -f interval=0.01 psana://exp=cxilx9320,run=46,repeat=true &
This is a dataset when we were aligning the gas cell
ami-local -b 1 -f interval=0.01 psana://exp=cxilx9320,run=47,repeat=true &
This is a dataset when we were aligning the pinhole
ami-local -b 1 -f interval=0.01 psana://exp=cxilx6520,run=244,repeat=true &
This is a dataset of SF6 static scattering
ami-local -b 1 -f interval=0.01 psana://exp=cxilx6520,run=214,repeat=true &
No gas, X-ray background, we might have damaged the pinhole because the background looks bad...

**********************************************************************

epixuhr:
- can download software
- one set of bare boards in lab1 (have been running it)

https://github.com/slaclab/epix-uhr-dev
instructions for running devGui above
kcu1500 firmware from https://github.com/slaclab/pgp-pcie-apps/releases

goals:
- get events flowing in the daq
  - check that timing link from the xpm works
- charge injection calibration software can be written now, but
  won't provide real data until we have real asic

emulator needs 6V digital and 6V analog power supply
will provide banana plugs
detector group will provide power supplies

will go in lab3 or FEE alcove (currently in lab1)

major differences between HR/UHR:
- 17kHz frames (eventually 35kHz)
- shape is different
- saci bus is different (register writes)
  o all the registers going to the asic are on saci
  o all other registers are on axi
- definition of saci registers is in

start from curent HR or from devGui? Lots of changes to make clocking
synchronous so not clear.

talk to pietro about technical stuff
lower priority than epixM and epixHR

**********************************************************************

xpp epixhr test:

- with lcls1 timing using new (top) xpm in 208
- use existing old-style wave8 bld
- not clear if we look like tmo/rix/tst

**********************************************************************

- 31,34,39 broken
- 32 epixhr timing
- 33,35-38,40-44 epixhr emulation
- 45 wave8

**********************************************************************

saxs/waxs meeting with tim van driel and andy aquila

other expts to analyze:

photonizing:
- yanwen xcs
- minitti photonizing

saxs/waxs:
gas phase
low intensity jet
detector calibration with constant sample and varying intensities to study systematics
2x2 high-q binning is a possibility

next on the list:

fluctuation saxs/waxs.  mark hunter old not-very-good data. new expt coming up.
spi: binning in q space because detector is not planar (needs fancy binning)

for next time:
look at differences with/without compression

**********************************************************************

fibers to mfx

lowest fodu in rack 2 in 208 rightmost cassette maybe?

need 6 strands
12 strands 8 available single-mode from mezzanine to mfx
are going to be in mfx rack
timescale: equipment arrives in 2 weeks, a month until it works
goes to main rack in hutch (rack 4) where networking is

**********************************************************************
