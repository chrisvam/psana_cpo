* dia, im2k0, batchtest, bluesky_lab3, xtcav sw, opal exposure, more ami/shmem cores, xtcdata throws

** teststand: kcu's to acc nodes (01,02,05,06), bring back ffb, fix ib manager and moving it to switch
* improved psana1 tests
* update rogue
* segment-level errors also go to control-gui
* mpi jobs not exiting if one core crashes
* put procstat in prometheus
* how to deal with pickle in the calibdb?
* protect against use of keywords as epics aliases, consider using namespacing. write a sanitizer for epicsarch files (use "import keywords"?)
* send clearreadout to pvadetectors
* support uniform 3D arrays in areadetector interface
* remove git passwords for relmanage
* eliminate opal lcls1->lcls2 timing toggling (larry, ben)
* multiseg epics (kwargs or collection segids?) (mona)
* jungfrau/epix dark shot pedestals
* psana1: implement idx using smd
* psana1: add epix firmware id in epix id's for phil
* upgrade jupyterhub or use on-demand?
* general test that all public detector attributes are method taking event with return type
* script to translate everything to h5
* pvadetector no timestamp matching "mode 0" (ric)
** support multiple cameras on one drp node
** one exe writing multiple files to improve performance
* move batch tests to slurm
** xtcdata duplicate names throw
* move away from afs (mv pdsdata/psalg repo)
* spares
** make everything work with Debug instead of RelWithDebInfo (valerio, ric)
* split off control package for reuse by hutch python (caf)
* move readout group config info from segment levels to ts
** xtcav recorder for mcc/acr (like running drp)
* optionally don't record selected detectors to disk
* 20GB/s filesystem
* mikhail: roentdek with quadanode
* fix camlink converter box with opal where only one strip works (requires camlink powercycle)
* daq window placement
* parallelized calibrations (mikhail)
* dan: ami timehistory
** state machine confusing (ric email) lower priority?
** psana/ami survive "reset"
** truncation damage keeps xtc iterable (ric)
* monitor deadtime per-lane (matt)
* improved grafana (like ganglia)
* reduction algorithms (ric)
* move other hutches to lcls2
* allocation of readout groups and devices, lanes
* shared drp node allocation mechanism
* move to gcc9 (valerio)
* xpplr8116 geom calib const access for py3
* make restart of hsdioc easier? (matt)
* MHz with real tmo analysis (mona)
* protect against illegal python names in dotted-hierarchy (e.g. ClinkFeb[0])
* create more platforms for procmgr
* practice power outage
** detnames/detnames-minus-e within python
* parallel jupyterhub with visualization
* daq2 chunking
* lcls2 private calibdir (mikhail)
* encoder for rix (caf)
* portable gpu detector corrections (kokkos/hip/openmp/opencl?)
* deploy releases everywhere (nersc, sdf, new/old psana1)
* have psana automatically detect which detectors are requested and only use those streams (mona)
* unified/integrated timetool calibration
* fix failing new-style psana1 tests (valerio)
* shifter mounting permissions (mona, johannes)
* problems on failure-modes confluence page
* disable IB in mpi until we understand intermittent crashes/warnings
* send multiple copy of events to shmem like lcls1
** small h5 ebeam/gasdet automatic storage
* small h5 ragged arrays
* meb (or later layer) broadcasts events to all clients (ami, python) (ric)
* read-while-write smallh5
* test daq sequences (caf)
* psana1 MTRX:V2 geometry (mikhail)
* update procServ (caf)
* slow updates need to obey deadtime (matt)
* setting msgdelay in timing system? (matt)
* syslog print throttling
* off-by-one (psana1/2)
* units support in det xface
* put psana2 (and psana1?) on conda-forge
* remote visualization/control
* calibdb dns issue (in travis macos build)
* in psana SRV callback only persist some fields to h5?
* once we have real data, work more on timetool calc in firmware
* dlopen for reduction alg
* test calibman/geo (mikhail)
* sdf/nersc calib-dir sync (wilko)
* psana1: continuous integration of py3 (jenkins)
* lcls2 configdb tools: history
* algorithms (drp/ana, e.g. beam-center finding)
* singularity at slac (mona)
* support more python versions
** SDF support including ARP
** python DRP? (ric, matt)
* fiber power readings everywhere (matt, tid)
* too many open files (mona)
* peaknet
* web xface for ami
* ami features: projection along curve, angular integration, epics import + looping, travis, xarray reading, mpi offline/evt-jumping, subgraphs, gui regression (run graphs), web xface, combine coarse+fine(timetool) timing, edge case: graph surgery global operation (worker/collector1and2) disconnect/reconnect, mask generation, josh requests, hsd time axis, hover to get docstrings, performance monitoring (prometheus)?, reopen displays on graph load, default size larger, default symbol, scatter plot number of points too large, hide "configure" menu running online, make "apply" more obvious/easy, reset kills ami (likely psana), plot vs readable time, hover over plot to get point (1D, 2D)
  - josh: free-form scan (no steps)
  - alex: set scan bin size limits (is histogram ok?)
  - josh: nanosecond xpcs: 2 pulse acqiris.  ratio of peak areas, correlation
    export results to ACR
  - josh: xpcs photon counting, working with chuck: accumulate statistics
    and then fit, number of photons per shot in histogram (talk to chuck
    and silke sxrm23). nicholas burdet (shared postdoc)
  - alex: manta camera (allied vision):  project, fit a peak (gaussian),
    plot vs. other variable,  ideally integrate over events, but
    difficult for ami. 30Hz.  run in mode with only a single shot per image.
* timetool (ben): tag to front end, fiber power, toggle xpmini->lcls2 timing, clear readout
* simpler interface for controlling teb/meb?
* xtcav daq recording epics variables as well (and also bld values?)
* xtcav/psocake py3 compatible

**********************************************************************

remote support

* containers
  - additional layer of complexity
  - singularity support @psana
  - onDemand makes it simpler to activate containers
  - permissions
  - size/quotas
  - jupyter onDemand support (copy sdf, kubernetes?)
  - user-generated containers? (complexity)
  - release goal: oip, make a container, use it @lcls and sdf in batch/interactively
* need to work on syncing permissions/acls/groups
* identical directory structure problem solved by using containers
* support both user ad-hoc envs and lcls official envs
