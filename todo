* lcls1 timing, rix scans, riccardo proj, tmo lcls1, ming-fu tpr/opal, pyxpm err, usleep, print statements, rix scans, rix failures, rix xpm pv access, andor psmon, gem-gdet, psana1 live-mode stat workaround, mfx fibers, more timing cables, epixM, epixUHR

weka stuff:
4.2=multicluster
july 4th
rhel8 weka server nodes (for multicluster): 
ethernet multicluster for s3df side:
s3 backend
ib multicluster (ffb) not yet, so nfs where ACLs don't work, could workaround by using ethernet
disappearing ib xface: working with nvidia. workaround: downgrade, but prevents moving to 4.3 (spring 2024). 4.1 supports rolling updates. 4.0.10.5 currently, july 4th
drp-srcf nodes upgrade: potentially OFED changes, need 1 drp-srcf-cmp* RHEL8 but won't move all the clients, and client weka versions 4.2 (depends on weka), test nodes not being used
data-mover/silke: decide friday workaround in place next week

mona proj:
- excluded detector list in confluence documentation
- eliminate filter_fn text from confluence documentation (now smd_callback)
- test live mode with full tmo.cnf
- more timing fibers for ric epixhr emulation
- dgramedit requires config_dgramedit argument. should it just be config_dgram?
- ttool background (work with ric)
- destination callback (conflicts with integrating detector?). work with silke,andy@txi
- epixM

s3df issues:
- test early next week
- test filesystems now and send notifications
- enable fairshare
- default 125 cores on milan
- seshu/amedeo/users agreeing with enabling ampere qos-preemptable
- silke: coordinator roles deprecated for job kill, since it's global (rubin, lcls...)
- fairshare?
- srun pmix "canary"
- open ports
- multicluster

tmo detectors:

Detectors to be ready in June: 2X FIMs (2 on KB1), ATM opal&piranha for IP1, FZP opal&piranha, HSDs, BLD, Laser wav8, IM5K4 

Detectors to be added by the end of calendar year 2023: two more FIMs on KB2 and IM6K4, ATM opal&piranha for IP2, another Laser wav8 for IP2

detector requests:
- (det) rixccd archon (installed in hutch march 2023, beam oct 2023?)
- superconducting BLD (fall 2022)
- (tid) high-rate mono-encoder
  o interpolated absolute encoder (march 2023)
  o high rate relative encoder (december 2023)
- dream hsd's in jan. 2023
- (unapproved) ikon (for SVLS, kristjan et al., early 2023, perhaps princeton mid-late 2023?)
- (tid, det?) k-microscope dld (test early summer 2022, production in early 2023?, beam summer 2023)
- ami roiarch with 3D (mikhail)
- (det) uxi needs more work from dan (lcls1)
- (standalone approved) (tid, det) tixel (tmo standalone rogue tmo, dec. 2022?, daq integration if looks good)
  o bojan in TID is doing the work
- (tid, det) epixM (integration prototype (no asic, no emulation) from tid beginning of june 2023? version with asic end of june?, 2 320kpx, 5kHz, commission april/june 2023, probably delayed),
  - no fixed gain mode, only one autorange mode AHL, some gain ambiguity that needs to be resolved
  - integrate in november 2022.  full cam in june 2023.
  - MTP24 broken out to 3 MTP8 fiber 6.4GB/s
  - 384*192*2 per camera (737MB/s per camera @5kHz)
- (tid,det) epixHR (prototype quad in march, start work sept 2022, 2Mpx, 5kHz, summer 2023) single MTP24 broken out to 3 MTP8 fibers: prototype in august 2022, beam in sept 2022 (and dec 22). quad in November 2022.  full cam in march 2023.  full 2M det commissioning june 2023.  quad is different than single.
  - 5 to 10 fibers pairs needs to be converted (use local xpm to generate
    multimode).  maybe 12 if edge-ml stuff.
  - 2 small epixHR's (140kpx?) single MTP12
- cookiebox (early 2024): guarantee 8 channels, hope for 16
- (will approve) (tid) opal replacement (s991? photometrics kinetix? giacomo thinks perhaps alvium?) https://www.alliedvision.com/en/products/alvium-configurator/alvium-1800-u/240/#_configurator
- (approved for standalone) (tid,det) timepix for rix: standalone mode in July 2022 (256x256 px,
  o get start-time and time-over-threshold, ~2kHz, could be faster)
  o working with anton tremsin at berkeley
- (unapproved) (det) varex xrd 4343 cameras for mec? (offered assistance, lcls1, needed april/may 2023)
- (unapproved) orca quest hamamatsu cxp camera? (lcls1, xpp? matt seaberg takahiro)
  https://www.hamamatsu.com/eu/en/product/cameras/qcmos-cameras/C15550-20UP.html
- (tid, det) small epixUHR (200x200): (emulator available end of feb. 2023) beam-test with daq april 2023 (35kHz).
  not identical to epixHR.
  - daq integration with firmware emulator in jan 2023?  prototype 1 asic in april 2023? beam time soon after
- andor iXon for UED (flexible, June/July 2023?)

* saxs/waxs data reduction (stefano)
* uniform (epics?) interface to detector config registers for config scans
* fluctuation saxs/waxs data reduction (stefano)
* manage running-condition dependent calibration constants (e.g. rate dependent) (tweak serial id?) (mikhail)
* improve destination callback (mona)
* monitoring new serial numbers for LCLS2 (mikhail)
* release DMA buffers earlier? currently released at same time as pebble (ric)
* small VDS "link files" seem to have absolute path to partN files. should be relative path.
* multi camlink cameras in a node
* configuration management/control for calibration/fexA/fexB
psana conflict between normalized integrating detectors and cube with destination callback? perhaps not a problem since integrating detector requires stable pump-probe time
* sphinx-like (read-the-docs?, makedocs, github?, confluence) auto-generated documentation (zach, ken, tom caswell) (mikhail)
* eliminate intermediate "panel" files on disk and put in database (for epix10k/jungfrau) (mikhail)
* calibration constant provenance (mikhail)
* h5 full-pathname issue in part0 files
* ami python editor returning array with one element into ScalarPlot generates no visible error (but see error in logs)
* understand/fix "384" intermittent libfabric ib completion queue issue (ric) (currently "fixed" by running eblf_pingpong)
* only 4 bits for xpm number in xpm remote link id (matt)
* more hsd's for simultaneous cookiebox/dream running (14+16 channels)
  but maybe not more hsd's if tixel works (weaver)
* not all errors show up in ami gui, e.g. missing attribute error (ddamiani)
* andor pedestal subtraction (mikhail)
* managing sharing: readout groups, drp nodes, cnf files, timing system connections
 - Chris Ford is working on a shared-DAQ resource allocation manager
 - Matt Weaver has this big-optical-switch ("BOS") which is principle allows more flexible use of the shared DAQ nodes
 - right now we have rix/tmo "official" setups.  we should probably create another one for acr, and then another one for one-off setups (like what Bill Schlotter is doing tomorrow).
 - make drp nodes more uniform?
* mask editor (mikhail)
* when running ami in daq detector list is intermittently not updated (ddamiani)
* have daq bluesky scan scripts default to "scan" as scan detector name to avoid psana crash (caf)
* ability for select-gui to allow teb's to be deselected (down to 1) (mikhail)
* epix100/epixhr deadtime not working (10Hz with epix100)
* ami 3D angular integration (mikhail)
* per stream and per run event/dmg counts (caf)
* better management of drp .service files (shared drive, puppet, kubernetes?)
** integrating detectors (daq and psana)
* configure andor with configdb?
* silver behenate fitting (including z-stagger) for panel positions (mikhail)
** benchmark SZ and other reduction for real expt data (mikhail)
  - can SZ handle gain-range detectors?
* make it harder to leave out timing system
* algorithm for non-quantized det-image charge sharing (mikhail)
* raise exception instead of crashing when idx/smd files are missing in psana1
* broadcasting non-epics slowupdate data (e.g. ttool bkdg)
* test psana2 live mode (mona)
* move encoder to tcp (caf)
* crystfel to psana geom (mikhail)
* ami mypy daemon is long-lived: can cross release boundaries
* configdb management for changing fex/teb decisions
* run lcls1 jungfrau at 120Hz (ddamiani, dan)
* timetool slowupdate background handling in psana (mona)
* procmgr performance with hundreds of nodes (caf)
* explore object stores for psana like ceph (mona)
* alias not in xtc file so more flexible
* select monitoring destination for particular events (ric)
* avoid missing 1 second of epicsarch data at beginning of run
* small daq teststand for automated continuous-integration tests?
* multisegment epicsinfo in pvadetector
* ami controlling DRP general ROI with masks via configdb? (mikhail)
* psmon only update one of multiplot (ddamiani)
* psana test to verify that all detector interfaces follow the rules
* psmon XYPlot format list of one item breaks multiple lines silently (ddamiani)
* more psmon examples for andy (ddamiani)
* off-by-one enabling automation (stoppers)
* xface for ami controlling detector params
* web display of ami plots
* mhz bld (matt)
* automatically push releases for lcls1/lcls2 to nersc/sdf/other
* makepeds/calibrun regression tests and simplification or freeze? (mikhail)
* opals for different daq's on same machine (larry+pcds)
* fix psana1 test release LD_PRELOAD hack caused by removal of
  ' -Wl,--copy-dt-needed-entries -Wl,--enable-new-dtags'
  in SConsTools/psdm_cplusplus.py for gcc48.
* psana dbase access for live mode list of files
* handle bad external timing better? (matt)
* generic detector calibration constants (e.g. manta)
* teststand: kcu's to acc nodes (01,02,05,06), bring back ffb, fix ib manager and moving it to switch
* make swmr work with small-h5
** for phil: configdb_readxtc and compare configs (ro, caf)
* lcls2 common-mode for panels in different gain ranges (mikhail)
* plims for lcls2 (mikhail)
* mpi jobs not exiting if one core crashes
* managing prometheus files in promdir
* put procstat in prometheus (or hard to duplicate all functionality?)
* how to deal with pickle in the calibdb? (mikhail)
* epicsarch support for strings (ric says it's hard)
* support damage in psana, including counting (mona)
* protect against use of keywords as epics aliases, consider using namespacing. write a sanitizer for epicsarch files (use "import keywords"?)
* send clearreadout to pvadetectors
* eliminate opal lcls1->lcls2 timing toggling (larry, ben)
* multiseg epics (kwargs or collection segids?) (mona)
* jungfrau/epix dark shot pedestals
* psana1: implement idx using smd
* correct setting for hyperthreading? (ric says important for MHz)
* psana2 idx mode
* psana1: add epix firmware id in epix id's for phil
* upgrade jupyterhub or use on-demand?
* general test that all public detector attributes are method taking event with return type (ro)
** support multiple cameras on one drp node
** one exe writing multiple files to improve performance
** xtcdata duplicate names throw
* move away from afs (mv pdsdata/psalg repo)
* spares
** make everything work with Debug instead of RelWithDebInfo (valerio, ric)
* move readout group config info from segment levels to ts
** optionally don't record selected detectors to disk
* mikhail: roentdek with quadanode
* fix camlink converter box with opal where only one strip works (requires camlink powercycle)
* daq window auto placement
* parallelized calibrations (mikhail)
* state machine confusing (ric email) lower priority?
* monitor deadtime per-lane (matt)
* improved grafana (like ganglia)
* move other hutches to lcls2
* allocation of readout groups and devices, lanes
* LCLS2 drop shots (bykiks evtcode 161)
* MHz with real tmo analysis including pre/post processing partitioning (mona)
* create more platforms for procmgr, perhaps with offsets for each hutch based on xpm? (caf)
* procmgr don't complain about undefined platform when all ports are specified (e.g. neh-base.cnf)
* practice power outage
* parallel jupyterhub with visualization (copy euxfel?) (riccardo/mona/wilko/valerio)
* portable gpu detector corrections (kokkos/hip/openmp/opencl?)
* deploy releases everywhere, containers? (valerio) (nersc, sdf, new/old psana1)
* unified/integrated timetool calibration
* fix failing new-style psana1 tests (valerio)
* shifter mounting permissions (mona, johannes)
* send multiple copy of events to shmem like lcls1
** small h5 ebeam/gasdet automatic storage (ro)
* small h5 ragged arrays
* meb (or later layer) broadcasts events to all clients (ami, python) (ric)
* read-while-write smallh5
* test daq sequences (caf)
* psana1 MTRX:V2 geometry (mikhail)
* update procServ (caf)
* slow updates need to obey deadtime (matt)
* setting msgdelay in timing system? (matt)
** syslog print throttling (caf)
* off-by-one (psana1/2) (ro)
* off-by-one support for low-rate devices
* units support in det xface
* put psana2 (and psana1?) on conda-forge
* calibdb dns issue (in travis macos build)
* in psana SRV callback only persist some fields to h5?
* once we have real data, work more on timetool calc in firmware
* dlopen for reduction alg
* scalable calib-fetch solution for shmem/drp
* sdf/nersc calib-dir sync (wilko)
* psana1: continuous integration of py3 (jenkins)
* lcls2 configdb tools: history, delete (into "trash" folder for recovery?) (ro)
* algorithms (drp/ana, e.g. beam-center finding)
* singularity at slac
* support more python versions
* S3DF support including ARP
* automatic users add to sdf lcls queue (wilko, valerio)
* python DRP? (valerio, ric)
* fiber power readings from timing system kcu's (matt)
* peaknet
* ami:
  - josh: nanosecond xpcs: 2 pulse acqiris.  ratio of peak areas, correlation
    export results to ACR
  - josh: xpcs photon counting, working with chuck: accumulate statistics
    and then fit, number of photons per shot in histogram (talk to chuck
    and silke sxrm23). nicholas burdet (shared postdoc)
* timetool (ben): tag to front end, fiber power, toggle xpmini->lcls2 timing, clear readout
* simpler interface for controlling teb/meb?
* xtcav daq recording epics variables as well (and also bld values?)
* xtcav/psocake py3 compatible

**********************************************************************

1 rack of computing for ffb
 - 1000 cores 8 nodes in 4u
ffb from 0.5PB to 1 or 2
sdf storage 6 jbods 1.5PB in 1 jbod in 4u for 50k + $overhead
enlarge sdf compute ?
switching to new srcf-sdf in april

**********************************************************************

2 filesystems: ffb (700TB) and offline (12PB)
with bigger disks can move ffb from 700TB to 1.4PB
700TB = 10 hours at 20GB/s

gpu:
2milan 32 high clock rate
4 nvidia a100

1 box 4 servers 2x64 core 1 box 512 cores 5K cores total

**********************************************************************

peppex cabling:
first cassette count from 1;
slot 2 hsd timing
slot 3 hsd leftmost
slot 4 hsd second from left

second cassette count from 1:
slot 4 hsd second from right
slot 5 hsd right
slot 6 opal

hsd timing plugged into xpm4 amc0 port 3 counting from 0

**********************************************************************

timepix discussion 06/06/22:

can use external trigger or free-running: we will use EVR-generated 120Hz trigger
main physics question to answer: can we separate one photon from two
will run at 120Hz
to synchronize with DAQ data can send extra ttl reset on a "slow" DAQ sequence to reset FPGA counter: this is more complex so don't do this
BNC or SMA with 50ohms
timing-in is easy: can use long exposure times and measure the time of the signal
jyoti will set up remote desktop to access anton's windows machine in the hutch via ICS

integrating comm protocol:
- timepix3 is event-driver (current one is timepix1)
- integrate timepix3
- use their fpga
- send jumbo udp packets on fiber. each photon is 6 bytes.
- suggests using their fpga timestamp

**********************************************************************

4000*2300*2*120=2GB/s (100Gbit enet)
triggering over cxp
fiber or bnc readout? (quad cxp6)
many other detectors (8 definite, 2 possible, varex)
lcls2: front-end box with timing and quad cxp6 in, pgp out
software timestamping for lcls1?  lcls2: firmware

**********************************************************************

rename roiarch?
masking and normalization (e.g. epix gaps)
arch outside image

- masks (define in ami?)
- 2d, 3d image
- q option with more parameters
- inherited/coupled parameters for multiple roiarch's

broken ami: pop, xtcav, hsd, photfind and linearity, xpp
good ami: blobfinder, hitfinder, flyscan, wf sideband sub, stepscan, wf integration vs delay stage, normalization of andor, ratio of peak areas and epics, calculator, complex, timestamp plot, export, reference plot

phil recommends divide by 16adu/kev
mikhails formula (sig-ped)/gain to get kev
det.gain returns something to divide or multiply by?
- silke multiplies by det.gain for epix100
epix10k 10 for high gain .01 for low gain
phil uploads adu/kev because mikhail divides
may be inconsistent with documentation
consistent handling (adu/kev or kev/adu) of:
- uploaded numbers
- file on disk deployed by makepeds
- value by det.gain
- documentation
epix100, rayonix for lcls1.  try to be consistent in lcls2
gain goes into pixel_gain file
mikhail has 0.06kev/adu

other detectors (jungfrau, epix10k): adu/kev->divide
epix100: ev/adu (.06kev/adu)->multiply (lcls1, other way round for lcls2)
many files have relative gains (around 1)
propose lcls1 "gain-factor": kev/adu

mark hunter:
institutional/personal pressure (sebastien helped prioritize)
collaborative vs. competitive access to resources

**********************************************************************

ami-local -b 1 -f interval=1 psana://exp=ueddaq02,run=569,dir=/cds/data/psdm/prj/public01/xtc/

dan and seshu
scripts vs ami
people develop their own libraries (personal/hutch):
  commonly used pieces moved into core
three categories:
  using existing boxes
  creating custom boxes
  detector guts
mask gain bits
thresholded image
managing libraries (avoiding creeping featurism)
editing library files with text editor
accessing calibconst: raw-pedestal (psana/psana/detector/UtilsEpix10ka.py
customizing epix calibration
controlling kwargs, e.g. cmpars
https://confluence.slac.stanford.edu/display/PSDMInternal/Detector+Interface+Proposal

**********************************************************************

rix high-rate risks:

moving DAQ to SRCF:
ric claus work
- high rates: hsd, wave8, piranha, bld
- selecting events in ami
matt: high rate bld
switching to LCLS2 timing
dan archon
s3df 

risks:
complexity of running detectors at different rates
high-rate mono-encoder (two?), high rate bld
using sequencer so andor can cleanly integrate over shots
new dets: epixM, high rate mono, archon, bld
practicing high-rate rix analysis in advance
managing which events AMI sees
s3df delays
IOC detectors (like andor) need new module from Kukhee

**********************************************************************

tmo detectors:

2 piranhas (fzp, atm) + 1 opal (fzp) + 1 opal (cvmi)
1 for dream atm + 1 spare
currently have 4 (2 cvmi, 1 atm, 1 fzp)
bld

rix priority:

2 andor newton
3 wave8
1 atm piranha
1 atm opal
hsd's?
bld
high-rate mono encoder and/or step-scans

**********************************************************************

general to do:
- recabling to srcf
  o use data volume to each node to figure out cabling
  o (optional) multi-camlink operation.
    - could program timing link on reboot
    - every process could set full link mask?
  o what should go through the bos?
- switching to LCLS2 timing (Dec. 1?)
- 33kHz BLD: ebeam, gmd, test asap. waiting for software development.
- commission s3df (but keep old system going too)
  o psana
  o smalldata
  o movers
  o mon-node calibdir
- spares (piranha in particular)
- migrate ctl nodes (move hsd processes, but not mono encoder)
- move timing crate from fee alcove to 208
- xtcav "nice" for acr?
- live mode


tmo to-do:
- when can we switch from opals to piranhas?
- (optional) multi-camlink running
- will fzp/atm opals run at same time?
- hsd fex overflow damage flagged?
- test hsd prescale raw waveform buffering
- move away from xpm0 to xpm2 for all tmo timing

tmo schedule:

[cvmi_opal_1, oct 27, nov 19] - retire date is negotiable, (maybe needs to be earlier?)
[cvmi_opal_2, oct 27, nov 19] - retire date is negotiable, (maybe needs to be earlier?)
[atm_opal,       oct 27, possibly run 22]
[fzp_opal,        oct 27, possibly run 22]
[atm_pirhana, oct 27, not planned]
[fzp_pirhana,  oct 27, not planned]

rix to-do:
- integrating-andor timestamping (I think Dan said this is fine as-is)
- accelerator burst-mode operation
- scans with burst-mode (one burst is one andor event)
- (somewhat optional) high-rate interpolated mono encoder for fly-scans
  o offline second-pass with interpolation could also work
  o step-scans with slow mono are another option

**********************************************************************

100kHz data volumes:

- wave8: 8waveforms*256samples/sample*2bytes/sample*0.1MHz = 400MB/s
  need to add FEX data too
- piranha: 2048px*2bytes/px*0.1MHz=410MB/s
- hsd full wf: 60000samples*2bytes/sample*0.1MHz = 12GB/s
  max waveform length appears to be ~60000 samples looking at xtc2 file?
  FEX needs to reduce it to 4GB/s (3x)
- bld: 336bytes/event*0.1MHz = 33MB/s
  (from xtcreader -f /cds/data/psdm/prj/public01/xtc/tmoc00118-r0222-s008-c000.xtc2)
- timing: 309bytes/event*0.1MHz = 31MB/s

recabling proposal:
- tmo timing (including pvadet, bld)
- rix timing
- (4) tmo camlink nodes
- (2) rix camlink nodes
- tmo/rix fim node (5 fims)
- (2) rix hsd nodes
- (7) tmo hsd nodes
- peppex hsd node
- (2) tmo/rix ami
- (2) tmo/rix teb
- (2) tmo/rix mebuser

total of 25 nodes
5 spares

**********************************************************************

3.10 errors:

qt errors in groupca/xpmpva
control.py qt setvalue wants int but getting float
piranha rogue errors

**********************************************************************

rix analysis

point detector APD via wave8, background subtract
direct andor integrate over shots and background subtract
vls andor no background
might want to run at 1Hz with bigger images
filtering (using the signal itself like APD, or other det: I0 from fim's, GMD, use goose trigger or bykik (laser/xray on/off))
binning in terms of delay (vitara) eventually use timetool
fly-scan with mono-encoder and bin
after binning (vitara, eventually timetool, mono encoder) do normalization (I0 kb fim's)
no peak-finders
could run droplet on vls andor (droplets may merge) background low enough that not necessary?
could we run the andors in different modes?  might be complex in burst mode

march: beam at the mono
april: beam in chemrix

mockup piranha at 33kHz by modifying opal?

propose 33kHz analysis as "worst case" (gets easier with bursts and integrating detectors)

**********************************************************************

according to bos:
cmp11 goes to amc0 port 5 (checked)
cmp12 goes to amc0 port 3 (checked)
cmp06 goes to amc0 port 6 (trusting bos aliases)

**********************************************************************

worries:
political assigning time
off-shift handling?  preemption or queue position?
reservations more wasteful
preemption latency

1 - reservations + head-of-queue-qos
2 - ffb preemption qos + head-of-queue-off-shift qos
3 - 4 layers of preemption

association: priorities based on partition/account/reservation/qos (either for preemption or position-in-queue)
slurmacctmgr command is used to manage priorities

**********************************************************************

outer product options:

need two: one with hsd channel itself, and one with piranha readout

- only do half of the symmetric outer product (with hsd channel itself)
- reduce the sample rate by ~4
- reduce window to 2us (12000 samples) 1us @1MHz
- use peaks (guess badness is 5+-3)
- replace with tixel eventually (which does thresholding)
  o may not use tixel because of the thresholding, depending on expt
- use pre-scaled full-waveform events for bkgd?

**********************************************************************

epixhr

include forced-low, forced-medium
for AHL, AML
new format for timing (2 lanes each with 2 asics, each with timing info)

saci bus lockup?
same firmware on receiving end?

**********************************************************************

txi hxr kpp measurements:
- postponed from april 29? may? june?
- 1kHz and 10kHz SC beam going through hxr undulators into txi
- wave8 daq (synced trigger)
- want 100Hz old hxr gasdet bld, gate width up to andy/philheimann
  o will do with unsynchronized epics.  silke says someone is working
    on this (called "GEM")
- NO ebeam bld (hxr bld (e.g. gasdet, some ebeam)  will be 120Hz?)
- epics
- no daq camera. use untimestamped controls camera, perhaps recorded in daq.
- need single mode fibers, may need cassettes
- timing in wave8 will be done earlier in tmo
- just wave8 and a bunch of epics variables
- other epics: power meter (andy, can get name from jyoti) controls uv sensitive diodes
  (alyssa, jyoti can provide epics names).  get gem-gdet names from silke (she
  said it might be working on may 11, 2023)

tickets:

https://jira.slac.stanford.edu/browse/LCLSECSD-1546
https://jira.slac.stanford.edu/browse/LCLSECSD-1874

**********************************************************************

rix fim 0,1 ioc's down
andor_dir ioc down
andors/manta need to move to lcls2 timing
no bld

**********************************************************************

xpp epixhr test:

xpp rack 2 elevation 22-25 ph5 (labelled on right): 4 free pairs on left?
xpp rack B950S-30 elevation 46 cassette labelled slot 3
strand 3,4 flipped in rightmost xpp cassette in 208, 1,2,5,6,7,8 good
leftmost xpp cassette in 208 strands 7,8 go to xpp hutch slot 1, fibers 7,8 (currently has something plugged in in the xpp hutch, but not in 208

- plug epixhr "data1" (fiber 1 on breakout cable, counting from 1)
  into rack 30 slot 3 fibers 5,6
- plug epixhr "data2" (fiber 2 on breakout cable, counting from 1)
  into rack 30 slot 3 fibers 7,8
- plug epixhr "timing" (fiber 4 on breakout cable, counting from 1)
  into rack 30 slot 3 fibers 1,2

in room 208 use top xpm right-most amc (0, I think) port 4 (counting from 0)
data1 goes into bos 1.6.7->5.2.5
data2 goes into bos 1.6.8->5.2.6

txi 3 cassettes free
txi rack b950s-13 elevation 43

for the xpp wave8 data silke writes:

XppSb2BeamMon	
239.255.24.75	XPP-SB2-BMMON	XPP Local

need to add second bld device with "ipimb" structure with new
multicast group
239.255.24.40
https://confluence.slac.stanford.edu/display/PCDS/Notes+on+Photon-Side+BLD+Generation
https://confluence.slac.stanford.edu/pages/viewpage.action?pageId=138790412

There also is SB3 (...76), but I think that is the device that is current an LCLS2 wave8. But check both, one should work, otherwise inquire with Vincent

**********************************************************************

conversation with xinxin

this problem
crashes
slowness
pyeditor not saving (inputs and outputs disappearing)

align pinhole and gas-cell
normalize with wave8
sum thresholded/normalized image to measure background (time-window or infinity)
plot summed number vs. x/y/tilt/roll for pinhole and gas-cell

use mean vs. scan

scatter sum vs. motor

want uniform background, compare quadrants (make difference close to zero)

ami-local -b 1 -f interval=0.01 psana://exp=cxilx9320,run=46,repeat=true &
This is a dataset when we were aligning the gas cell
ami-local -b 1 -f interval=0.01 psana://exp=cxilx9320,run=47,repeat=true &
This is a dataset when we were aligning the pinhole
ami-local -b 1 -f interval=0.01 psana://exp=cxilx6520,run=244,repeat=true &
This is a dataset of SF6 static scattering
ami-local -b 1 -f interval=0.01 psana://exp=cxilx6520,run=214,repeat=true &
No gas, X-ray background, we might have damaged the pinhole because the background looks bad...

**********************************************************************

epixuhr:
- can download software
- one set of bare boards in lab1 (have been running it)

https://github.com/slaclab/epix-uhr-dev
instructions for running devGui above
kcu1500 firmware from https://github.com/slaclab/pgp-pcie-apps/releases

goals:
- get events flowing in the daq
  - check that timing link from the xpm works
- charge injection calibration software can be written now, but
  won't provide real data until we have real asic

emulator needs 6V digital and 6V analog power supply
will provide banana plugs
detector group will provide power supplies

will go in lab3 or FEE alcove (currently in lab1)

major differences between HR/UHR:
- 17kHz frames (eventually 35kHz)
- shape is different
- saci bus is different (register writes)
  o all the registers going to the asic are on saci
  o all other registers are on axi
- definition of saci registers is in

start from curent HR or from devGui? Lots of changes to make clocking
synchronous so not clear.

talk to pietro about technical stuff
lower priority than epixM and epixHR

**********************************************************************

xpp epixhr test:

- with lcls1 timing using new (top) xpm in 208
- use existing old-style wave8 bld
- not clear if we look like tmo/rix/tst

**********************************************************************

- 31,34,39 broken
- 32 epixhr timing
- 33,35-38,40-44 epixhr emulation
- 45 wave8

**********************************************************************

saxs/waxs meeting with tim van driel and andy aquila

other expts to analyze:

photonizing:
- yanwen xcs
- minitti photonizing

saxs/waxs:
gas phase
low intensity jet
detector calibration with constant sample and varying intensities to study systematics
2x2 high-q binning is a possibility

next on the list:

fluctuation saxs/waxs.  mark hunter old not-very-good data. new expt coming up.
spi: binning in q space because detector is not planar (needs fancy binning)

for next time:
look at differences with/without compression

**********************************************************************

fibers to mfx

lowest fodu in rack 2 in 208 rightmost cassette maybe?

need 6 strands
12 strands 8 available single-mode from mezzanine to mfx
are going to be in mfx rack
timescale: equipment arrives in 2 weeks, a month until it works
goes to main rack in hutch (rack 4) where networking is

**********************************************************************

elegant handling of bad configurations: what can we do?

(bad triggering)

two messy cases:

wrong l0delay: crashes, deadtime
too large gate_ns: missed trigger (some detectors, like piranha)
  - remove the detectors from partition in a binary-search fashion to
    learn who is responsible, then look at settings of that detector, in particular
    gate_ns

piranha may need its own l0delay because of hutch->srcf trigger round-trip (7us each way?)
l0delay is in us, so instead of 99, perhaps 79? lower l0delay increases pressure
on buffering but OK at 100kHz.

hsd: shouldn't drop triggers because it supports overlapping windows.  even if
     data volume gets large deadtime should keep triggering orderly.
piranha: drop a trigger and get off by one
   could set batcher-event-builder timeouts, but would have to change dynamically
   with gate_ns and trigger-rates

100ms different timestamp for the andor (when triggering at 10Hz)

(should test, but hopefully under control) slow code/datatransfer: deadtime, associated with right det

**********************************************************************

norm=g5=evtcode272 (SEQCODES 0Hz?) (one image every 30 seconds in ami?)
dir=g6=evtcode276 (SEQCODES 102Hz?) (0.5Hz in ami)

switch norm andor to evtcode273 (SEQCODES 102Hz): behaves better

**********************************************************************

We're looking to replace our 50 gallon gas water heater with a heat-pump water heater.  There is physical space (we might move it about 4 feet to an area with 8 feet of vertical height).  I believe we have electrical capacity for a 240V 15A model from a nearby sub panel (about 10 feet away).  We would like a 10 year warranty, if possible.  You can see some photos here:  https://photos.app.goo.gl/qUsDruW5kQVZBKpFA.  We would need some help (e.g. documentation) to get rebates from Peninsula Clean Energy as well as permits.  Thanks!

noise (rheem)
power
wall strap
warranty
rebates
condensate
leak detection
15a vs 30a vs 120v
code/permits

recommended electricians by wizard plumbing: jeff lewis electric, bayshore electrical
603-867-8248
recirculation pump?

bayshore plumbers also recommended bayshore electric

electrician rob ortiz 669 292 9707 recommended by united plumbing. liked him.

shoreway plumbing:
julio and hugo (plumbers installing water heater)
jose: 650-670-2036

try other atm's
call boa to get list of symbols

lab3: 68.2F later: 66.8F
pietro's lab: 69.5F later: 69.5F
fee alcove outside racks: 71.3F
fee alcove inside rack: 81.4F

lab3 nodes:
drp-tst-dev008, dev004, acc05, acc02, acc01,
daq-tst-dev02 (add kcu?), dev03 (add kcu?), dev06 (hsd)

redwood city heat-pump water heater
vicky sherman 650-780-7472 vsherman@redwoodcity.org climateactionplan@redwoodcity.org
