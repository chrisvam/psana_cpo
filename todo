mfx+txi node count

* mudit access, rix issues, erik data fetch, 4U servers, mfx+txi node count, cassettes, slurm daq, spack daq, hxr xtcav fiber, mfx fibers, fix teb->control timeouts, epixM charge injection, phil per-run grafana link, timestamps for unaligned, tmol1030922 crashes (daq_crash), hsd b2:a, psmon error on non-default port, one copy of devgui tools, tmo daq restarts morning of jun20, loaner fmc134 hsd (no carrier), xcsl1018322 slow jobs (run table and arp around run 240, june 15), atca crates, order hsd's/camlink, mfx epix100, rittal, fix rix hsd's, test cmp006 jira with camlink, ebeam/pcav BLD (pcav timing charlie shu should time it in, ebeam unclear), archon calib, ctl002 network, intg_det slowupdate/l1 prio, txi kpp (grp7 grafana)

erik:
- default ps_smd_n_event
- datasource batch_size 200
- tmox1009422 run 26

epix100 issues: config-scan, deadtime, calibration, configuration

archon:
- *** pixel gain 3.65eV/ADU
- calib shape should be like raw (i.e. include fake pixels)
- image excludes the fake pixels
- *** common mode should be optional with a kwarg
- *** x,y,z (fake pixels return x,y,z NaN?)
  o it is a dense 2D array (no gaps)
- *** standard bad-pixel-mask where fake pixels are masked
- *** user-mask-bit bit saying "fake pixel"
- *** mask out the timestamp pixels or mark as bad so they don't get used in
  common mode (talk to Dan D to figure out which are timestamp pixels)
- dan needs to update hutch firmware for pixel timestamps
- triggering at 0.5Hz synced to beam is tough. do scientists really need
  this or is binning OK?

gabriel projects:
- validating rix daq
- txi beamtime
- adding wave8 to teststand
- archon calibration

julian projects:
- kcu pcie problem
- timing link lock failures
- bucket hopping
- eye scan
- rix hsd repair
- programming hsd firmware over pcie

dan projects:
- lcls2 jungfrau
- ami2 caproto.server export
- archon timestamp
- xinxin ami2
- vincent kwargs

chris ford projects:
- new tpr in daq-det-evr01 (all in the same rack) furthest rack from door
  (timing in first rack, both xpm and lcls1)
- triggerDelay errors to gui (maybe grafana (bigger job))
- sysctl.conf management
- epix logbook prec issue raised by silke
- qrixs camlink boxes
- epixm server
- install spark arrestors
- kcu in drp-srcf-gpu*
- spack testing
- slurm testing
- test ipmi everywhere
- cmp15,16 (or 14,15) bad kcu or ib
- fiber optics inventory/order
- camlink box inventory/order
- check bios settings
- cabling
  o new hsd's epixhremu with kcu1500 timing fanout in srcf
- and also running the epixhremu?
  o work in configdb for SZ compression configuration bits?

pressio gpu projects:
- robert: lc compression interface
- stefano: pipeline automation, GPU performance, post SAXS/WAXS results
- jiannan: commit improved encoding code, 
- batching?
  o pressio: set Flush=true returns data object
  o 

dan projects:
- ami2 tests for lcls1
- delay line detector
- alice green ami graph for lcls1
- vincent filter box
- jungfrau 16M
- mec varex, xpp orca quest? depending on priorities
- clean up ami tmo/rix errors
- in psana SRV callback only persist some fields to h5?

mikhail projects:
- calibrun alvium support (including only 1)
- only show cnf configs in control_gui editor
- psana2 det kwargs to instantiation
- daq configdb editor doesn't save properly when hex value specified
- psana1 det.image kwargs to det.calib
- lcls2 jungfrau det.calib performance (do it "perfectly")
- improve psana1 calib const fetch scaling
- new psana1 release (pick up kludge fix)
- why doesn't psana mpi crash when a live-mode file read error happens?
- mask-generator gui to set per-asic gain modes for epixHR/epixM?
  (silke says per-pixel gain modes typically done in python)

caf projects:
- go through all drp-srcf-cmp*, eb*, mon* and see which ones have broken IPMI
  web interfaces
- fix daqstate?
- put platform number in elog (along with other daq values) for grafana
  damage url construction
- learn how to use the the supermicro bios command-line tool and make
  sure that all nodes have correct bios settings
- "configdb history" printing local time instead of UTC
- inventory nodes with bad ipmi web interface
- fix hyperthreading
- syslog print throttling
- setup spare drp nodes (onsite)
- help repair fee alcove cooling (onsite)
- mpo12 to mpo8 (onsite)
- kcu for drp-gpu (onsite)
- kcu-xpm in fee (onsite)

matt projects:
txi run
run julian's eye scan for all steps in hsd chain xpm->hsd->kcu including jesd
  - official software changes in july
  - can we kludge something so we can debug broken tmo/rix hsd's
    without interfering with running?
rog0 idea
pcav/ebeam bld
gpu

ric projects:
- epixm: config scans, run trigger, pixel gain cfg
- pvadetector calib support (user specified serial number, dettype, etc.)
- drp gpu
- psana grafana
- libsz to roi switch (and easily change roi config params)
- epixhremu to 20 nodes
- multicamlink drp

riccardo proj:
- epixuhr
- test the whole hsd chain for hsd_2/hsd_5 with eye-scan
  o move a loopback along the steps in the fiber path until eye-scan
  o could also swap hsd fibers in the bos to see if it's a kcu/hsd problem
    o set up a dedicated loopback channel in the bos
- one copy of devgui tools
- update peppex hsd's
- mfx fee spec test
- repair tmo/rix hsd's
- spare drp nodes (timing, wave8, camlink, hsd)
- mpo12 to mpo8
- teststand piranha

mona proj:
- rix continuous beam integ detector offset
- poor tmo scaling
- 
- unaligned detectors timestamp and timesort
- slurm instead of procmgr
  o multiple cnf "inheritance"
- psana2 scaling issue
- archon calibration
- psana sz/lz decompress with pressio?
- integrating detector offset (e.g. n-3 to n+2 to rix andor)
- (daq) two threshold droplet alg for xpp-he
- (daq) cube pattern
- (daq) epixuhr
- daq spack
- destination callback (conflicts with integrating detector?). work with silke,andy@txi

s3df projects:
- smd0 exclusive node
- lcls:default account vs scavenger?
- tom grant: 400Gbit/s network limit (can be less) compatible with LCLS2 20GB/s. when? half of milan's are on "unreliable" srcf2 network (haven't brought it back).  hardware is flakey.  switches are not behaving well. 60 new milan nodes (most are rubin's)
- kill large memory on interactive nodes (victor elmir working on it w/email)
- heterogeneous jobs too slow: require backfill, constraints slow things down, "immediate flag helps"
- will x7333 be published anywhere?
- gpu direct storage
- when do we enable preemption? (june or july, needs disk)
- ssh failures (OOM)
- 24/7-support via xHELP: have a s3df-specific new number (x7333)
- jupyter memory usage:
  o default approach: use Yee's one-liner?
  o fix load-balancing (currently get iana 07/08 move to maybe 6 nodes?)
    psana is 01/02
  o kill idle sessions? 
  o memory limit 50GB?
    - can we get message to user automatically?
  o limit cores (someone took 16 cores, only 24 total). suggest limit of 8
  o limit CPU usage of a particular process.  suggest limit of 1 hour
    realtime 8 hours CPU time (running on 8 cores)
  o reduce weka cores on interactive nodes?
- AMD tuning guide
- ptychography gpu reservation (and workshop)
- kubernetes ACLs
- ipmi access to reboot? (wilko)
- public_html for psreldev
- default jupyter to batch and add note
- how does slurm gather cores for mpi?
- "perazzo" sharing (needs a scalable way of calling the "delta" function)
  o delta-function (allocated vs. used) takes a long time to compute: can create a denial-of-service of slurm since it's slow
  o "forcing" people into preemptable when they exceed their allocation
- jupyter automatic queue selection
- weka: read-while-write (permissions fixed with 4.2.4)
- silke: coordinator roles deprecated for job kill, since it's global (rubin, lcls...)

tmo detectors:

Detectors to be ready in June: 2X FIMs (2 on KB1), ATM opal&piranha for IP1, FZP opal&piranha, HSDs, BLD, Laser wav8, IM5K4 

Detectors to be added by the end of calendar year 2023: two more FIMs on KB2 and IM6K4, ATM opal&piranha for IP2, another Laser wav8 for IP2

detector requests priority update:
(new: UED 1080px epix10ka)
meeting with sebastien/james/georgi on nov. 20, 2023
- (mostly done) rixccd
- superconducting phase-cavity (close) and ebeam BLD for lcls2
- small txi epixHR (beginning of 2025)
- rix high-rate mono-encoder
- UED 1080Hz epix
- mfx jungfrau16M (a year)
- tmo tixel (running standalone in february?)
  - magnetic bottle expts could use existing detector
  - would be nice to replace a dream delay-line-detector
    with tixel? need a larger one. not obligated to users
    higher priority than big txi epixHR
  - could replace k-microscope delay-line detector
- rix axis sxr-60 camera (chemrix spectrometer)
  - smaller pixel than epix
  - can't use andor: flange-mount makes grazing-incidence difficult
  - could happen before "the shutdown" (july 2025)
  - highest priority for new instrument for rixs experiments
  - plan is to implement as an IOC (who? dan? if tid could do it that would be great)
- big txi epixHR (big one delayed until aug 2025)
- rix epixM (depends on approved experiments)
- rix k-microscope delay-line-detector (depends on approved experiments)
  o should we use the dream dld instead?
  o communicate with Bob Schoenlein
- epixUHR (testing and in XPP, production not until late 2026)
- ued andor ixon (the camera used in the old daq? but maybe not)
  - maybe an IOC because of long integration times
  - might not be too bad if it's an andor
  - get more information
- opal replacement?
  - imperx? AD supports IOC with pgp-camlink. not super-fast (60Hz-100Hz)
  - maybe not necessary since gige can be timestamped at 100Hz? needs testing
    can talk to dougie about some data taken with high rate exit-slit manta
    (progress from Dan and MikeB)
- (previously approved for "standalone running") rix timepix
  - tixel does same sort of thing
  - done by anton
  - no definitive plans
  - josh turner is supposed to work with anton
  - successor to epixM to get to high rates (or tixel)
- (previously "unapproved") mec varex (a user group is bringing one to MEC)
- (previously "unapproved") xpp orca

- (det) rixccd archon (installed in hutch march 2023, beam oct 2023?)
- superconducting BLD (fall 2022)
- (tid) high-rate mono-encoder
  o interpolated absolute encoder (march 2023)
  o high rate relative encoder (december 2023)
- (unapproved) ikon (for SVLS, kristjan et al., early 2023, perhaps princeton mid-late 2023?)
- (tid, det?) k-microscope dld (test early summer 2022, production in early 2023?, beam summer 2023)
- (det) uxi needs more work from dan (lcls1)
- (standalone approved) (tid, det) tixel (tmo standalone rogue tmo, dec. 2022?, daq integration if looks good)
  o bojan in TID is doing the work
- (tid, det) epixM (lorenzo, integration prototype (no asic, no emulation) from tid beginning of end of august 2023 with asics? 2 320kpx, 5kHz),
  - no fixed gain mode, only one autorange mode AHL, some gain ambiguity that needs to be resolved
  - integrate in november 2022.  full cam in june 2023.
  - MTP24 broken out to 3 MTP8 fiber 6.4GB/s
  - 384*192*2 per camera (737MB/s per camera @5kHz)
- (tid,det) epixHR (prototype quad in march, start work sept 2022, 2Mpx, 5kHz, summer 2023) single MTP24 broken out to 3 MTP8 fibers: prototype in august 2022, beam in sept 2022 (and dec 22). quad in November 2022.  full cam in march 2023.  full 2M det commissioning june 2023.  quad is different than single.
  - 5 to 10 fibers pairs needs to be converted (use local xpm to generate
    multimode).  maybe 12 if edge-ml stuff.
  - 2 small epixHR's (140kpx?) single MTP12
- tmo cookiebox (early 2024): guarantee 8 channels, hope for 16
- mfx jungfrau 16M
- (will approve) (tid) opal replacement (s991? photometrics kinetix? giacomo thinks perhaps alvium?) https://www.alliedvision.com/en/products/alvium-configurator/alvium-1800-u/240/#_configurator
- (approved for standalone) (tid,det) timepix for rix: standalone mode in July 2022 (256x256 px,
  o get start-time and time-over-threshold, ~2kHz, could be faster)
  o working with anton tremsin at berkeley
- (unapproved) (det) varex xrd 4343 cameras for mec? (offered assistance, lcls1, needed april/may 2023)
- (unapproved) orca quest hamamatsu cxp camera? (lcls1, xpp? matt seaberg takahiro)
  https://www.hamamatsu.com/eu/en/product/cameras/qcmos-cameras/C15550-20UP.html
- (tid, det) small epixUHR (200x200): (emulator available end of feb. 2023) beam-test with daq april 2023 (35kHz).
  not identical to epixHR.
  - daq integration with firmware emulator in jan 2023?  prototype 1 asic in april 2023? beam time soon after
- andor iXon for UED, alex reid (flexible, June/July 2023?) a.k.a. EMCCD
  - slow (1s to 30s) and integrates
- axis-sxr-60-std 36MPx 26Hz camera request from Kristjan in RIX
  https://www.axis-photon.com/streak-camera/axis-sxr-60-36mpix-soft-x-ray-scmos-camera/

* grafana to epics alarm handler
* after-the-fact lcls2 daq alias in database (both configdb and detnames)
* share simple python code between ami2/psana1/psana2 (amityping, psmon, "HPolar")
* datasource kwarg to suppress fetching of calib constants for mikhail's mask editor (mona)
* split daq/ana conda envs (valerio)
* better error when exposure time > trigger rate.
* hsd raw/fex counters so james can alarm on low raw rates (weaver)
* calibconst for lcls1 for ami2 (e.g. jungfrau)
* saxs/waxs data reduction (stefano)
* uniform (epics?) interface to detector config registers for config scans
* fluctuation saxs/waxs data reduction (stefano)
* manage running-condition dependent calibration constants (e.g. rate dependent) (tweak serial id?) (mikhail)
* ami roiarch with 3D (mikhail)
* improve destination callback (mona)
* psana-gpu (mona)
* event counts in logbook for phil
* monitoring new serial numbers for LCLS2 (mikhail)
* release DMA buffers earlier? currently released at same time as pebble (ric)
* running daq with missing (high-rate?) bld (ric claus)
* check for differing types in smd.sum (gives "ndarray not contiguous" err) (mona)
* separate (decouple) daq/ana envs (valerio)
* detnames with wrong syntax produces incorrect output
  (e.g. the ";" here: "detnames exp=rixc00121;run=81") (mikhail, mona)
* running daq with damaging pvadet (ric claus)
* fix daq timeout behavior where we sometimes get damage from many nodes  (ric claus)
* converting grafana values into epics alarms
* configuration management/control for calibration/fexA/fexB
* psana conflict between normalized integrating detectors and cube with destination callback? perhaps not a problem since integrating detector requires stable pump-probe time
* sphinx-like (read-the-docs?, makedocs, github?, confluence) auto-generated documentation (zach, ken, tom caswell) (mikhail)
* eliminate psana1 calibdir in favor of database (mikhail)
* eliminate intermediate "panel" files on disk and put in database (for epix10k/jungfrau) (mikhail)
* use only one of nlohmann_json/rapidjson
* better error when running psana with small data but not setting PS_SRV_NODES  and when number of cores are incorrect (mona, mikhail)
* didn't get obvious error when writing to unwriteable wave8 pv register (MasterEnable) (riccardo, mona, ric)
* event counts in elog for phil. jira ecs-3843 (chris ford, mikhail)
* calibration constant provenance (mikhail)
* resurrect psana1 unit tests ("scons test") (mikhail)
* ami python editor returning array with one element into ScalarPlot generates no visible error (but see error in logs)
* when rix piranha had small number of dma buffers (like opal) we often, but not always, got 100% deadtime and BEB timeouts.  feels like buffers not configurerd correctly when deadtime happens (ric, matt, mona, riccardo)
* why doesn't psana mpi crash when a live-mode file read error happens? (mikhail)
* force inclusion of all necessary TREX data for xtcav
* utility to put lcls2 h5 files in time-order (mona? mikhail? caf? riccardo?)
* understand/fix "384" intermittent libfabric ib completion queue issue (ric) (currently "fixed" by running eblf_pingpong)
* only 4 bits for xpm number in xpm remote link id (matt)
* support libfabric newer "tcp" provider (old one was "sockets") (ric)
* more hsd's for simultaneous cookiebox/dream running (14+16 channels)
  but maybe not more hsd's if tixel works (weaver)
* not all errors show up in ami gui, e.g. missing attribute error (ddamiani)
* andor pedestal subtraction (mikhail)
* managing sharing: readout groups, drp nodes, cnf files, bos connections
 - make drp nodes more uniform?
* mask editor (mikhail)
* when detector list is long in control-gui, can't hit "apply" button  (mikhail)
* when running ami in daq detector list is intermittently not updated (ddamiani)
* have daq bluesky scan scripts default to "scan" as scan detector name to avoid psana crash (caf)
* ability for select-gui to allow teb's to be deselected (down to 1) (mikhail)
* epix100/epixhr deadtime not working (10Hz with epix100)
* ami 3D angular integration (mikhail)
* per stream and per run event/dmg counts (caf)
* better management of drp .service files (shared drive, puppet, kubernetes?)
** integrating detector "exposure time" psana param (mona)
* configure andor with configdb?
* silver behenate fitting (including z-stagger) for panel positions (mikhail)
** benchmark SZ and other reduction for real expt data (mikhail)
  - can SZ handle gain-range detectors?
* make it harder to leave out timing system
* algorithm for non-quantized det-image charge sharing (mikhail)
* raise exception instead of crashing when idx/smd files are missing in psana1
* broadcasting non-epics slowupdate data (e.g. ttool bkdg)
* control_gui not showing up on right screen in tmo (ami, procstat do) (mikhail)
* move encoder to tcp (caf)
* crystfel to psana geom (mikhail)
* ami mypy daemon is long-lived: can cross release boundaries
* configdb management for changing fex/teb decisions
* timetool slowupdate background handling in psana (mona)
* procmgr performance with hundreds of nodes (caf)
* explore object stores for psana like ceph (mona)
* select daq meb monitoring destination for particular events (ric)
* avoid missing 1 second of epicsarch data at beginning of run
* small daq teststand for automated continuous-integration tests?
* multisegment epicsinfo in pvadetector
* ami controlling DRP general ROI with masks via configdb? (mikhail)
* psmon only update one of multiplot (ddamiani)
* psana test to verify that all detector interfaces follow the rules
* psmon XYPlot format list of one item breaks multiple lines silently (ddamiani)
* more psmon examples for andy (ddamiani)
* off-by-one enabling automation (stoppers)
* xface for ami controlling detector params
* web display of ami plots
* mhz bld (matt)
* bld data structure populated automatically from epics
* automatically push releases for lcls1/lcls2 to nersc/sdf/other
* makepeds/calibrun regression tests and simplification or freeze? (mikhail)
* opals for different daq's on same machine (larry+pcds)
* fix psana1 test release LD_PRELOAD hack caused by removal of
  ' -Wl,--copy-dt-needed-entries -Wl,--enable-new-dtags'
  in SConsTools/psdm_cplusplus.py for gcc48.
* psana dbase access for live mode list of files
* handle bad external timing better? (matt)
* generic detector calibration constants (e.g. manta)
* teststand: kcu's to acc nodes (01,02,05,06), bring back ffb, fix ib manager and moving it to switch
* make swmr work with small-h5
** for phil: configdb_readxtc and compare configs (ro, caf)
* lcls2 common-mode for panels in different gain ranges (mikhail)
* address psana/daq prometheus issues described on confluence (mona/ric)
* managing prometheus files in promdir
* put procstat in prometheus (or hard to duplicate all functionality?)
* how to deal with pickle in the calibdb? (mikhail)
* epicsarch support for strings (ric says it's hard)
* support damage in psana, including counting (mona)
* protect against use of keywords as epics aliases, consider using namespacing. write a sanitizer for epicsarch files (use "import keywords"?)
* send clearreadout to pvadetectors
* eliminate opal lcls1->lcls2 timing toggling (julian, matt)
* multiseg epics (kwargs or collection segids?) (mona)
* jungfrau/epix dark shot pedestals
* psana1: implement idx using smd
* correct setting for hyperthreading? (ric says important for MHz)
* psana2 idx mode
* psana1: add epix firmware id in epix id's for phil
* upgrade jupyterhub or use on-demand?
** one exe writing multiple files to improve performance
** xtcdata duplicate names throw
* move away from afs (mv pdsdata/psalg repo)
* spares
** make everything work with Debug instead of RelWithDebInfo (valerio, ric)
* move readout group config info from segment levels to ts
** optionally don't record selected detectors to disk
* fix camlink converter box with opal where only one strip works (requires camlink powercycle)
* daq window auto placement
* parallelized calibrations (mikhail)
* monitor deadtime per-lane (matt)
* move other hutches to lcls2
* MHz with real tmo analysis including pre/post processing partitioning (mona)
* create more platforms for procmgr, perhaps with offsets for each hutch based on xpm? (caf)
* parallel jupyterhub with visualization (copy euxfel?) (riccardo/mona/wilko/valerio)
* portable gpu detector corrections (kokkos/hip/openmp/opencl?)
* drp gpu
* deploy releases everywhere, containers? (valerio) (nersc, sdf, new/old psana1)
* unified/integrated timetool calibration
* fix failing new-style psana1 tests (valerio)
* send multiple copy of events to shmem like lcls1
** small h5 ebeam/gasdet automatic storage (ro)
* meb (or later layer) broadcasts events to all clients (ami, python) (ric)
* update procServ (caf)
** syslog print throttling (caf)
* off-by-one support for low-rate devices
* units support in det xface
* put psana2 (and psana1?) on conda-forge
* in psana SRV callback only persist some fields to h5?
* once we have real data, work more on timetool calc in firmware
* scalable calib-fetch solution for shmem (have it for drp)
* sdf/nersc psana1 calib-dir sync (wilko)
* psana1: continuous integration of py3 (jenkins)
* lcls2 configdb tools: history, delete (into "trash" folder for recovery?) (ro)
* algorithms (drp/ana, e.g. beam-center finding)
* singularity at slac
* support more python versions
* fiber power readings from timing system kcu's (matt)
* timetool (ben): tag to front end, fiber power, toggle xpmini->lcls2 timing, clear readout

**********************************************************************

peppex cabling:
first cassette count from 1;
slot 2 hsd timing
slot 3 hsd leftmost
slot 4 hsd second from left

second cassette count from 1:
slot 4 hsd second from right
slot 5 hsd right
slot 6 opal

hsd timing plugged into xpm4 amc0 port 3 counting from 0

**********************************************************************

txi hxr kpp measurements: (aquila)
https://confluence.slac.stanford.edu/pages/viewpage.action?spaceKey=L2SI&title=RP+testing
- scheduled around halloween '23?
- 1kHz and 10kHz SC beam going through hxr undulators into txi
- wave8 daq (synced trigger)
- want 100Hz old hxr gasdet bld, gate width up to andy/philheimann
  o will do with unsynchronized epics.  silke says someone is working
    on this (called "GEM")
- NO ebeam bld (hxr bld (e.g. gasdet, some ebeam)  will be 120Hz?)
- epics
- no daq camera. use untimestamped controls camera, perhaps recorded in daq.
- need single mode fibers, may need cassettes
- timing in wave8 will be done earlier in tmo
- just wave8 and a bunch of epics variables
- other epics: power meter (andy, can get name from jyoti) controls uv sensitive diodes
  (alyssa, jyoti can provide epics names).  get gem-gdet names from silke (she
  said it might be working on may 11, 2023)

tickets:

https://jira.slac.stanford.edu/browse/LCLSECSD-1546
https://jira.slac.stanford.edu/browse/LCLSECSD-1874

**********************************************************************

lab3: 68.2F later: 66.8F
pietro's lab: 69.5F later: 69.5F
fee alcove outside racks: 71.3F
fee alcove inside rack: 81.4F

lab3 nodes:
drp-tst-dev008, dev004, acc05, acc02, acc01,
daq-tst-dev02 (add kcu?), dev03 (add kcu?), dev06 (hsd)

**********************************************************************

epixhremu timing link status

34: bad rxclock:  no fiber
37: looked good (linkup) -3.49
38: bad rxclock -3.69 (fixed by plugging/unplugging)
39: down: no fiber
40: rxclock 0: -26.22
41: bad rxclock: -3.9
42: bad rxclock: -3.6 (fixed by plugging/unplugging)

**********************************************************************

rix device issues:
piranha 71kHz and 100dt at 10kHz
mr2k1: no timing sub-window (just "no EVR" message)
hsd_0,1 (and dt with hsd2,3 eventually)
encoder
only chemrix wave8
manta not downstream of xpm?

recorded rixx1003721 run 169 with
71kHz: timing, wave8
10Hz: opal,manta
1Hz: vls,norm,dir

recorded rixx1003721 run 171 with
71kHz: timing, wave8, hsd_0, hsd_1
10Hz: opal,manta,vls,norm,dir,mr2k1(untimestamped),exsmanta

norm/dir rog 5, evtcode 272 (1Hz)
vls,opal,manta rog 6, evtcode 276 (10Hz, gated)
timing,wave8 rog 2, evtcode 284 (33kHz, gated)

c**********************************************************************

rix doesn't see problems at the moment. so maybe xpm0 is happy?

shortterm:

1. clean fibers and change transceivers between xpm0 and xpm2
2. separate xpm's in the atca crate
3. eject xpm2/xpm4 (should be no difference with fru_deactivate)
   (less likely since 6,4 go down) change transceivers
   (less likely since 6,4 go down) clean fibers
4. remove xpm2 (xpm0->xpm4->xpm6) (currently 6 is driven by 2)
5. swap xpm's with fee alcove
6. new version of software broke it? (unlikely since rix is working)
7. more RF isolation with air resistance modules
8. pyxpm software overwriting registers
9. only xpm's with front-panel inputs have trouble? (or "second tier leaf")

xpm4,5,6: noRTM
xpm0,2,3: RTM

longterm:

- reproduce in fee alcove (by unplugging fiber from ACR)
- instrument firmware
  o PLL input frequency drifting out of range (lock status)
  o power glitching
- is it a lingering temperature effect?  (maybe reproducible in the
  teststand)

possible partial solutions:
- reset state machine that gets stuck (we kludged a fix once with a loopback)
  Matt has an idea here.

timing issues:
- spares (including rtm's)
- txlinkreset (firmware)
- xpm link glitches (firmware)
- toggle between xpmmini/lcls2
- 25kHz fiducial
- xpm ports won't lock to themselves
- no link lock on timing KCUs
- distribute knowledge more widely
- general diagnostic techniques for understanding problems like the above (e.g. how do we know a PLL input is within a required frequency range?)
- want to try: synchronous retransmission (no clock domain crossing)
  o can be done without trying to understand firmware
- not just them telling us what to do: deeper committment
  o needs gert

**********************************************************************

yee preemption proposal:

only preemptable jobs: one "floating partition" called "shared"

have a floating partition for all facilities: "lcls", "suncat" is a facility
  - lcls float partition would be called lcls-float
  - suncat float partition would be called suncat-float
  - float partitions can have pre-emptable and non-preemptable jobs and higher priority jobs
  - floating partition: not locked down to specific hosts (can mix clusters)
  - the resources for the facility-float partition can be capped (only use 80%)

lcls real-time jobs would preempt jobs in "shared" partition first and
then other lower-priority lcls jobs

problem: lcls jobs are not preemptable

look into suspend-preemption again?

goal:
- guarantee 3000 core jobs start in <1 minute. need a solution by january
  or february
options:
- reservations (like a partition)
- use current system
  o helps to not demand exclusive node (but can have noisy-neighbor)
  o no guarantee
- "island" milan partition with suspend/kill preemption with lcls jobs? (what we did previously)
  o two sub-partitions of milan: prioq, normq
  o would need to set this up with suncat
  o breaks "sharing" model of s3df

for (2):
suppose lcls only has milan 88 nodes
suppose offline jobs have filled up milan 88 nodes with norm qos jobs offline
- 88 purchased (lcls milan nodes)
- 40 purchased (milan nodes from others)
yee solution: realtime jobs have higher qos that could preempt norm qos ("compute requirement" set different for different repos)
this qos only affect queue position, but can also set to preempt

jan. 12 discussion

- force things that are preemptable (like suncat) to be together?
- amedeo's accounting approach has long time-constant, but guaranteed
  1 minute to execution needs short time constant
- can automatically requeue or suspend
- yee's concrete proposal: a slurm plug-in to manage preemption
  (using default qos plug-in).  each job is assigned a qos
  serves three function (1) set priority within the queue
  (2) set resource limits a group of jobs can use (also
  constrainted by coact, to try to limit the number of non-preemptable
  jobs by a facility) (3) define what other jobs the job in question
  can preempt
  o yee's group is trying to write a custom plug-in to manage qos:
    e.g. lcls:on-shift can preempt lcls:normal
  o regarding (2) number of jobs is limited a group TRES

jan 19

yee's two points:
(1) need to limit the number of non-preemptable jobs, in particular the
    single-core jobs which can scatter across nodes
(2) how do we preempt the jobs and limit the resources used by preemptable
    jobs
yee's proposal:
(1) limit every facilities use of non-preemptable jobs to what
    they have purchased (e.g. LCLS is limited to 88 nodes of
    non-preemptable jobs, or half of 176 nodes)
    - coact can help with this
    - is this a hard limit, or does it have a long time-constant?
      o a hard limit for a repo (enforced by slurm), but not at the
        multiple-repo level
        (multi-repo is not enforced by slurm, but coact so a long
	 time-constant to see if we've crossed the 88 node
	 threshold)
(2) LCLS defines the order of preemption of experiment repos
(add-on) could extend this to support cross-facility preemption

concrete example of yee's proposal:
4 expts: exp1 (on-shift), exp2 (off-shift), exp3 (normal), exp4 (normal)
- set exp1 to have high-priority for queue-placement
  o implemented by setting QOS=on-shift for exp1
- have a sequence of job-QOS's: preemptable, normal, off-shift, on-shift
  o could have "normal" jobs be preemptable, although could create
    issues with sharing with others like rubin
  o reuse murali's stuff to automatically get on-shift/off-shift settings
    - uses the experiment runs within calendar URAWI start/end time
    - can this handle last-minute changes?  could add some buffer at
      the edges or manually override (sub-czars could do this?)
  o setting "on-shift" setting for a repo is a permission, the
    on-shift expts could specify on-shift or normal QOS in
    job submission script
    - there can be a default QOS
    - if a job is set to a non-permissible QOS job will
      currently fail (Silke would like it to switch to lower QOS
      automatically)
- we will try suspend preemption within the milan partition
  o ****** memory ****** is a worrisome issue, but nodes will get larger
    SSDs in 3 months (several TB)
    - expect 2-10GB/s maybe limited by kernel, so 512GB would take 1.5min
      but happens in parallel, so hopefully OK
- each experiment repo would set an allocation (a hard limit!) that
  would limit the number of cores (enables multiple on-shift expts)
- beamline staff could tweak repo allocations?  may need sub-czar
  (operator) privileges

**********************************************************************

brainstorm with ric about epixm issues:

summary:
(1) we'll try to filter out [] () in dgram.cc
- can't translate square brackets to xtc2. leave them in configdb so yaml goes along for the ride, take them out 
- current method: remove square brackets in xtc, put in for rogue python, remove for xtc2
(2) "no detname" error from PythonConfigScanner.
(3) matt has educated us about how he handle the yaml files

**********************************************************************

weka options:

"posix write hole"

- * range file locks: perfect, but doesn't work easily bbcp
  o people use acls as locks as a hack
- * mount flag:
  o currently allow clients to go faster than the rest of the cluster
  o turn this off.  performance will decrease.  sometimes drop to 1GB/s
  o atomic-write: serializes write threads mount option
  o a new mount option on the writer
- mpi i/o with collective hints. would currently still work
- * use similar tricks to mpi i/o (like romeo)
  - dentry_max_age_positive=0 (mount option)
- 4k writes? decreases probability

https://docs.weka.io/fs/mounting-filesystems#for-all-clients-types

**********************************************************************

need: timing fw fixes, rogue6, updated rogue packages
- high priority rogue: camlink, wave8
- high priority non-rogue: hsd, xpm, ts
- maybe lcls2-pgp-pcie-apps (only write to this for epixhr at the moment?)
- low priority: epix*, high-rate-encoder, tixel

if we update wave8, does controls also need to move to new rogue?

need julian (currently ill) to sign-off on wave8 timing changes

could move one hutch at a time to make it easier

deploy new kcusim

**********************************************************************

fim1 bucket-skipping
fim0 missing phase2 of config (fixed itself?)
hsd's had wrong group (and hence wrong l0delay) but fixed itself?
cmp002/010 timing link not locked (power cycle)
opal/piranha: hanging on startup (fixed by rebooting)
timing crate unplugged
powercycle camlink nodes 012 027 with filesystem hangs?
no ami client window (reboot cmp029)
mono_trig crash (new tpr.ko)

**********************************************************************

ued 1080hz epix10ka:
- two opportunities: march or july

- still switching between daq's?
- how many fibers?
- andor ixon/emccd

ixon/emccd:
- run epix or ixon/emccd, not both
- ideally want shot-to-shot data e.g. shot-by-shot timetool
- 30second integration times are long. can we trigger that slowly?
- dan thinks it is sdk2 which means it should be easier (like newton?)

**********************************************************************

- production experience with incremental steps
- good to demonstrate that the lab can be successful with the detectors with the incremental step
- also include cost estimates with current technology

**********************************************************************

epixm:
- xtc2 format
- descramble in software (using _dataDebug.py or untested C code)
- what scans det group needs (pedestal? charge injection?)
  o pedestals: 5 steps for HR, fewer for M (maybe none?)
  o charge injection
- management of run triggers vs. daq triggers (hopefully
  reuse what matt already has)
- ask matt about the beb timeout value

**********************************************************************

variants of kcu1500 firmware (also c1100 in future, u200 avelo, ):
6/10/15/25Gbps
pgp2/pgp4/next-gen (next-gen won't fit in the same fpga)
all have pause sent to front-end (could be ignored by the feb)
beb/nobeb
cameralink (cpo thinks we did this "wrong": avoid in future
            no timing links should go directly to kcu1500
	    except for coaxpress-over-optical)
one build for all speeds
  o except for next-gen >pgp4 25Gbps?
  o could be a project for julian

**********************************************************************

rittal

192.168.0.190
setup smtp for email communications
set laptop to (for example) 192.168.0.195
work with jim runner at slac on these things
X4 on the back
v3.17.30.2 is latest version will come with new unit
will include power unit as well
modbus 4 units:
 - unit1: cmc cpu (software to control lcp)
 - unit2: lcp (needs lcp4 software) .tar (over ethernet)
mark meyer acct mgr, steven engineering
fans, flowmeters, cmc, front display

found instructions here:
https://www.rittal.de/downloads/rimatrix5/security/CMCIII/Update_manual_V4_en.txt

on this page:
https://www.rittal.com/de_de/rimatrix-downloads/index.asp?kat=security&subk=70

used en7 on mac with Valerio's usbc->enet board

beeps obnoxiously for about 1 minute

**********************************************************************

can/should we remove the cpu from the l1accept path
batching, and keep events in time-order
clarify which lines involve cpu/gpu/kcu
do we "throw the switch"?
test payloads of user-defined size trigger by l1
multi kcu to one gpu
root complex

can we use unified memory?

line 96 talks to kcu: puts gpu memory addr into kcu. a gpu addr, which is good!
rdsrv415 has a pcie chassis that we could consider
iommu allows to do stuff with wide-open security settings
line 123 is for GPU, ryan says "permissions"
line 132 and 137 are for specific registers
line 152 is a gpu register
line 159 is the gpu writing to the kcu
line 164 gpu polling
line 173 transfers data back to kcu from gpu
line 159 tells the kcu we are done with the buffer

hwWritePtr is a GPU pointer
hwWriteStart is a KCU pointer

gpu gen4 16lane 50GB/s
gen4 c1100 bifurcated 8+8 matches the GPU, 25GB/s for 8 lanes

cuda_graphs branch has test_dma.cu with loop and measurements
not sure if "cuda graphs" is working

**********************************************************************

rix c_atmopal dropped disable problem

swapped these two connections:

5.1.5-1.3.7 cmp027 sfp1-1 to xpm5_amc0_sfp6
4.8.4-5.8.6 cmp012 sfp1-1 to xpm5_amc0_sfp3

**********************************************************************

tmo atmpiranha drop problem

5.1.1-5.8.1 cmp026 sfp1-1 to xpm6_amc0_sfp4 (atmopal)
5.4.4-1.3.3 cmp007 sfp1-1 to xpm6_amc0_sfp6 (atmpiranha)

x2150

**********************************************************************

sudo launchctl disable system/com.apple.icloud.searchpartyd
sudo kill -STOP 5753
