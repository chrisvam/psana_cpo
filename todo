loops, ami config
* xfracas, users mtg, ami2 det.config, txi diodes, tmo gmd, srcf, mono enc, drp ipmi, peppex hsd134 segfault, daq-tmo-hsd-02, srcf tx/rx swap
s3df: sudo, shared accounts, group permissions, access to offline /scratch from ffb

ami issues: 
- (bug) need a new amityping
- (bug) missing "dict" type in drop-down list of types for pyedit
- (bug) one time exported pyeditor box had no terminals (not reproducible)
- (inconvenience) can't export multiple pyeditor boxes to same py lib file
  - have to change EventProcessor class so no conflicts in py lib file
- (bug) exported py edit boxes need Reset when edited-in
- (documentation) documentation: use Node/CtrlNode or other Node?
- (feature?) beginrun in py edit box should be passed run: allows people to
  make their own detectors, passing flags?
- (bug) toggling between managed-library graphs:
  File "/cds/home/c/cpo/git/ami/ami/flowchart/NodeLibrary.py", line 45, in addNodeType
    raise Exception("Node type name '%s' is already registered." % name)
Exception: Node type name 'Thresh2900' is already registered.


commissioning plan docs (19309-a4002): https://docs.google.com/spreadsheets/d/e/2PACX-1vR_f7tpWeLufEHW12w9aPW64wv73TcaZ4MOIjRyjjxOY3s75vnJ3yRPD9OycXIyTmCDjIhKKexzHeXm/pubhtml

srcf to do:
- bos/fiber handling
- fiber purchase (done)
- new detector requests (see below)
- atm edge-finding for piranha and feedback
- rogue relative-path script (done, valerio used package-name mangling)
- daq kafka bus all-files-started
- buffering settings (test SIGSUSPEND for each device)
*- monitoring decision for low-rate dets
- protect against hsd firmware unhappy if too many fex peaks (matt)
* psana damage handling
- remember detector readout groups in cnf? (done by caf)
- epicsarch not writing out 1MHz l1accepts (or goes in slow readout group)
- xpm fanouts in srcf (maybe not urgent since only piranha's need them?)
*- auto coupled deadtime xpm register support in timing system python
- time in detectors
*- need slowupdate to obey deadtime? (messes up Ric's control-Z test of deadtime in srcf)
*- move all tmo devices to xpm:2
- move the pyxpm processes/network to SRCF
  - could also have local connections from e.g. rix nodes to the rix atca crate
  - short term: DAQ:NEH:XPM/DAQ:TMO:HSD/RIX:FIM:W8/MR4K2:FIM:W8/MR3K4:FIM:W8/MR2K4:FIM:W8
    goes in the gateway RW
- ami-settable data reduction params (e.g. thresholds for hsd)
*- scale to many drp's
- high rate dets
  - piranha
*  - bld
  - hsd
  - wave8
  - epixhr
- low rate det
  - epicsarch
  - pvadet
- roentdek alg (done by mona)
- chunking (done by caf)
*- python drp
- rixccd
- psana
  - live-mode
*- dynamic resource mgmt
*- automatic off by one detection

detector requests:
- (det) rixccd archon (working in daq sep 2022, beam 2 months later)
- superconducting BLD (fall 2022)
- (tid) high-rate mono-encoder (jan. 2023)
- (unapproved) ikon (for SVLS, kristjan et al., early 2023, perhaps princeton mid-late 2023?)
- (tid, det?) k-microscope dld (test early summer 2022, production in early 2023?, beam summer 2023)
- h5 full-pathname issue in part0 files
- (det) uxi needs more work from dan (lcls1)
- (standalone approved) (tid, det) tixel (tmo standalone rogue tmo, dec. 2022?, daq integration if looks good)
  o bojan in TID is doing the work
- (tid, det) epixM (start work nov 2022, 2 320kpx, 5kHz, commission april/june 2023),
  - no fixed gain mode, only one autorange mode AHL, some gain ambiguity that needs to be resolved
  - integrate in november 2022.  full cam in june 2023.
  - MTP24 broken out to 3 MTP8 fiber 6.4GB/s
  - 384*192*2 per camera (737MB/s per camera @5kHz)
- (tid,det) epixHR (start work sept 2022, 2Mpx, 5kHz, summer 2023) single MTP24 broken out to 3 MTP8 fibers: prototype in august 2022, beam in sept 2022 (and dec 22). quad in November 2022.  full cam in march 2023.  full 2M det commissioning june 2023.  quad is different than single.
  - 5 to 10 fibers pairs needs to be converted (use local xpm to generate
    multimode).  maybe 12 if edge-ml stuff.
  - 2 small epixHR's (140kpx?) single MTP12
- (will approve) (tid) opal replacement (s991? photometrics kinetix? giacomo thinks perhaps alvium?) https://www.alliedvision.com/en/products/alvium-configurator/alvium-1800-u/240/#_configurator
- (approved for standalone) (tid,det) timepix for rix: standalone mode in July 2022 (256x256 px,
  o get start-time and time-over-threshold, ~2kHz, could be faster)
  o working with anton tremsin at berkeley
- (unapproved) (det) varex xrd 4343 cameras for mec? (offered assistance, lcls1, needed april/may 2023)
- (unapproved) orca quest hamamatsu cxp camera? (lcls1, xpp? matt seaberg takahiro)
  https://www.hamamatsu.com/eu/en/product/cameras/qcmos-cameras/C15550-20UP.html
- (tid, det) small epixUHR (200x200): beam-test with daq april 2023 (35kHz).
  not identical to epixHR.
  - daq integration with firmware emulator in jan 2023?  prototype 1 asic in april 2023? beam time soon after
- andor iXon for UED (flexible, June/July 2023?)

* manage running-condition dependent calibration constants (e.g. rate dependent) (tweak serial id?) (mikhail)
* monitoring new serial numbers for LCLS2 (mikhail)
* small VDS "link files" seem to have absolute path to partN files. should be relative path.
* swapping slurm jobs script (ro)
* update aes-stream-driver to 64-bit so we can unpin rogue
* eliminate intermediate "panel" files on disk and put in database (for epix10k/jungfrau) (mikhail)
* calibration constant provenance (mikhail)
* ami python editor returning array with one element into ScalarPlot generates no visible error (but see error in logs)
* understand/fix "384" intermittent libfabric ib completion queue issue (ric) (currently "fixed" by running eblf_pingpong)
* long eb connect times in rix (ric)
* managing sharing: readout groups, drp nodes, cnf files, timing system connections
 - Chris Ford is working on a shared-DAQ resource allocation manager
 - Matt Weaver has this big-optical-switch ("BOS") which is principle allows more flexible use of the shared DAQ nodes
 - right now we have rix/tmo "official" setups.  we should probably create another one for acr, and then another one for one-off setups (like what Bill Schlotter is doing tomorrow).
 - make drp nodes more uniform?
* mask editor (mikhail)
* ability for select-gui to allow teb's to be deselected (down to 1) (mikhail)
* epix100/epixhr deadtime not working (10Hz with epix100)
* ami angular integration
* per stream and per run event/dmg counts (caf)
** integrating detectors (daq and psana)
* configure andor with configdb?
** benchmark SZ and other reduction for real expt data (mikhail)
  - can SZ handle gain-range detectors?
* make it harder to leave out timing system
* broadcasting non-epics slowupdate data (e.g. ttool bkdg)
* move encoder to tcp (caf)
* crystfel to psana geom (mikhail)
* ami mypy daemon is long-lived: can cross release boundaries
* configdb management for changing fex/teb decisions
* run lcls1 jungfrau at 120Hz (ddamiani, dan)
* move TMO to xpm:2 (like RIX)
* reduction algorithms
* turning low rate data into per-shot high rate (e.g. ttool background, encoder) ami+psana (mona)
* elegant early shutdown of psana with smd0 irecv for mtip? (mona)
* ami2 array thresholding (ro)
* procmgr performance with hundreds of nodes (caf)
* explore object stores for psana like ceph (mona)
* alias not in xtc file so more flexible
* psana datasource for drp (mona)
* teb having memory allocation problem after too many deallocate's (ric)
* select monitoring destination for particular events (ric)
* avoid missing 1 second of epicsarch data at beginning of run
* small daq teststand for automated continuous-integration tests?
* multisegment epicsinfo in pvadetector
* ami controlling DRP ROI via configdb? (seshu)
* twocanary (toucan) in srcf
* code/results migration to/from nersc/sdf
* psmon only update one of multiplot (ddamiani)
* psana test to verify that all detector interfaces follow the rules
* psmon XYPlot format list of one item breaks multiple lines silently (ddamiani)
* more psmon examples for andy (ddamiani)
* consider adding "safe" Xtc::alloc with max-size argument to avoid fixed-buf overruns (or maybe in higher layer)
* off-by-one enabling automation (stoppers)
* xface for ami controlling detector params
* web display of ami plots
* mhz bld (matt)
* uniform (epics?) interface to detector config registers for config scans
* automatically push releases for lcls1/lcls2 to nersc/sdf/other
* hsd test pattern mode (matt)
* makepeds/calibrun regression tests and simplification or freeze? (mikhail)
* opals for different daq's on same machine (larry+pcds)
* fix psana1 test release LD_PRELOAD hack caused by removal of
  ' -Wl,--copy-dt-needed-entries -Wl,--enable-new-dtags'
  in SConsTools/psdm_cplusplus.py for gcc48.
* psana dbase access for live mode list of files
* handle bad external timing better? (matt)
* deadtime-per-detector in grafana
* generic detector calibration constants (e.g. manta)
* teststand: kcu's to acc nodes (01,02,05,06), bring back ffb, fix ib manager and moving it to switch
* make swmr work with small-h5
** for phil: configdb_readxtc and compare configs (ro, caf)
* archon for rix (dan)
* improved psana1 tests (ro)
* lcls2 common-mode for panels in different gain ranges (mikhail)
* plims for lcls2 (mikhail)
* mpi jobs not exiting if one core crashes
* managing prometheus files in promdir
* put procstat in prometheus (or hard to duplicate all functionality?)
* how to deal with pickle in the calibdb? (mikhail)
* epicsarch support for strings (ric says it's hard)
* support damage in psana (including counting)
* protect against use of keywords as epics aliases, consider using namespacing. write a sanitizer for epicsarch files (use "import keywords"?)
* send clearreadout to pvadetectors
* support uniform 3D arrays in areadetector interface
* remove git passwords for relmanage
* eliminate opal lcls1->lcls2 timing toggling (larry, ben)
* multiseg epics (kwargs or collection segids?) (mona)
* jungfrau/epix dark shot pedestals
* psana1: implement idx using smd
* correct setting for hyperthreading? (ric says important for MHz)
* psana2 idx mode
* psana1: add epix firmware id in epix id's for phil
* upgrade jupyterhub or use on-demand?
* general test that all public detector attributes are method taking event with return type (ro)
** support multiple cameras on one drp node
** one exe writing multiple files to improve performance
** xtcdata duplicate names throw
* move away from afs (mv pdsdata/psalg repo)
* spares
** make everything work with Debug instead of RelWithDebInfo (valerio, ric)
* move readout group config info from segment levels to ts
* optionally don't record selected detectors to disk
* mikhail: roentdek with quadanode
* fix camlink converter box with opal where only one strip works (requires camlink powercycle)
* daq window placement
* parallelized calibrations (mikhail)
* state machine confusing (ric email) lower priority?
* monitor deadtime per-lane (matt)
* improved grafana (like ganglia)
* move other hutches to lcls2
* allocation of readout groups and devices, lanes
** shared drp node allocation mechanism (caf)
* LCLS2 drop shots (bykiks evtcode 161)
* MHz with real tmo analysis including pre/post processing partitioning (mona)
* create more platforms for procmgr
* practice power outage
* parallel jupyterhub with visualization
* portable gpu detector corrections (kokkos/hip/openmp/opencl?)
* deploy releases everywhere, containers? (valerio) (nersc, sdf, new/old psana1)
* unified/integrated timetool calibration
* fix failing new-style psana1 tests (valerio)
* shifter mounting permissions (mona, johannes)
* send multiple copy of events to shmem like lcls1
** small h5 ebeam/gasdet automatic storage (ro)
* small h5 ragged arrays
* meb (or later layer) broadcasts events to all clients (ami, python) (ric)
* read-while-write smallh5
* test daq sequences (caf)
* psana1 MTRX:V2 geometry (mikhail)
* update procServ (caf)
* slow updates need to obey deadtime (matt)
* setting msgdelay in timing system? (matt)
* syslog print throttling (caf)
* off-by-one (psana1/2) (ro)
* off-by-one support for low-rate devices
* units support in det xface
* put psana2 (and psana1?) on conda-forge
* calibdb dns issue (in travis macos build)
* in psana SRV callback only persist some fields to h5?
* once we have real data, work more on timetool calc in firmware
* dlopen for reduction alg
* scalable calib-fetch solution for shmem/drp
* sdf/nersc calib-dir sync (wilko)
* psana1: continuous integration of py3 (jenkins)
* lcls2 configdb tools: history, delete (into "trash" folder for recovery?) (ro)
* algorithms (drp/ana, e.g. beam-center finding)
* singularity at slac
* support more python versions
* S3DF support including ARP
* automatic users add to sdf lcls queue (wilko, valerio)
* python DRP? (valerio, ric)
* fiber power readings everywhere (matt, tid)
* peaknet
* ami:
  - josh: nanosecond xpcs: 2 pulse acqiris.  ratio of peak areas, correlation
    export results to ACR
  - josh: xpcs photon counting, working with chuck: accumulate statistics
    and then fit, number of photons per shot in histogram (talk to chuck
    and silke sxrm23). nicholas burdet (shared postdoc)
* timetool (ben): tag to front end, fiber power, toggle xpmini->lcls2 timing, clear readout
* simpler interface for controlling teb/meb?
* xtcav daq recording epics variables as well (and also bld values?)
* xtcav/psocake py3 compatible

**********************************************************************

1 rack of computing for ffb
 - 1000 cores 8 nodes in 4u
ffb from 0.5PB to 1 or 2
sdf storage 6 jbods 1.5PB in 1 jbod in 4u for 50k + $overhead
enlarge sdf compute ?
switching to new srcf-sdf in april

**********************************************************************

2 filesystems: ffb (700TB) and offline (12PB)
with bigger disks can move ffb from 700TB to 1.4PB
700TB = 10 hours at 20GB/s

gpu:
2milan 32 high clock rate
4 nvidia a100

1 box 4 servers 2x64 core 1 box 512 cores 5K cores total

**********************************************************************

peppex cabling:
first cassette count from 1;
slot 2 hsd timing
slot 3 hsd leftmost
slot 4 hsd second from left

second cassette count from 1:
slot 4 hsd second from right
slot 5 hsd right
slot 6 opal

hsd timing plugged into xpm4 amc0 port 3 counting from 0

**********************************************************************

timepix discussion 06/06/22:

can use external trigger or free-running: we will use EVR-generated 120Hz trigger
main physics question to answer: can we separate one photon from two
will run at 120Hz
to synchronize with DAQ data can send extra ttl reset on a "slow" DAQ sequence to reset FPGA counter: this is more complex so don't do this
BNC or SMA with 50ohms
timing-in is easy: can use long exposure times and measure the time of the signal
jyoti will set up remote desktop to access anton's windows machine in the hutch via ICS

integrating comm protocol:
- timepix3 is event-driver (current one is timepix1)
- integrate timepix3
- use their fpga
- send jumbo udp packets on fiber. each photon is 6 bytes.
- suggests using their fpga timestamp

**********************************************************************

4000*2300*2*120=2GB/s (100Gbit enet)
triggering over cxp
fiber or bnc readout? (quad cxp6)
many other detectors (8 definite, 2 possible, varex)
lcls2: front-end box with timing and quad cxp6 in, pgp out
software timestamping for lcls1?  lcls2: firmware

**********************************************************************

rename roiarch?
masking and normalization (e.g. epix gaps)
arch outside image

- masks (define in ami?)
- 2d, 3d image
- q option with more parameters
- inherited/coupled parameters for multiple roiarch's

broken ami: pop, xtcav, hsd, photfind and linearity, xpp
good ami: blobfinder, hitfinder, flyscan, wf sideband sub, stepscan, wf integration vs delay stage, normalization of andor, ratio of peak areas and epics, calculator, complex, timestamp plot, export, reference plot

phil recommends divide by 16adu/kev
mikhails formula (sig-ped)/gain to get kev
det.gain returns something to divide or multiply by?
- silke multiplies by det.gain for epix100
epix10k 10 for high gain .01 for low gain
phil uploads adu/kev because mikhail divides
may be inconsistent with documentation
consistent handling (adu/kev or kev/adu) of:
- uploaded numbers
- file on disk deployed by makepeds
- value by det.gain
- documentation
epix100, rayonix for lcls1.  try to be consistent in lcls2
gain goes into pixel_gain file
mikhail has 0.06kev/adu

other detectors (jungfrau, epix10k): adu/kev->divide
epix100: ev/adu (.06kev/adu)->multiply (lcls1, other way round for lcls2)
many files have relative gains (around 1)
propose lcls1 "gain-factor": kev/adu

mark hunter:
institutional/personal pressure (sebastien helped prioritize)
collaborative vs. competitive access to resources

**********************************************************************

ami-local -b 1 -f interval=1 psana://exp=ueddaq02,run=569,dir=/cds/data/psdm/prj/public01/xtc/

dan and seshu
scripts vs ami
people develop their own libraries (personal/hutch):
  commonly used pieces moved into core
three categories:
  using existing boxes
  creating custom boxes
  detector guts
mask gain bits
thresholded image
managing libraries (avoiding creeping featurism)
editing library files with text editor
accessing calibconst: raw-pedestal (psana/psana/detector/UtilsEpix10ka.py
customizing epix calibration
controlling kwargs, e.g. cmpars
https://confluence.slac.stanford.edu/display/PSDMInternal/Detector+Interface+Proposal

**********************************************************************

rix high-rate risks:

high-rate mono-encoder
using sequencer so andor can cleanly integrate over shots
new dets: epixM, high rate mono, archon, bld
practicing high-rate rix analysis in advance
managing which events AMI sees
