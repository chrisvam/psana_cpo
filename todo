* epix100, fiber tx/rx (mpo), epixhr2m perf, multiopal, papers, ami roi (epixhr)

tmo mtg:
- epix100
  - timing scan
- roentdek
- mhz psana h5

srcf:
- bos/fiber handling
- move the pyxpm processes/network to SRCF
  - could also have local connections from e.g. rix nodes to the rix atca crate
  - short term: DAQ:NEH:XPM/DAQ:TMO:HSD/RIX:FIM:W8/MR4K2:FIM:W8
    goes in the gateway RW
- high rate dets
  - piranha
  - bld
  - hsd
  - wave8
  - epixhr
- low rate det
  - epicsarch
  - pvadet
- roentdek
- chunking
- python drp
- ami settable roi's
- rixccd
- psana
  - live-mode
- resource mgmt
- off by one

epix100:
- tasks: timing scan, calib, run trigger, monitoring stream, takepeds
  o run trigger: l0, accept trigger: l1, main reason: uniform temp
  o run executable, make it work (first-order) with simple config
- questions:
  o (matt) yaml file converter
  o (matt) timing scan: ~rixopr/scripts/epixhr/
  o (matt) how do you handle different submodules (surf) for kcu/epixhr
  o (matt) why different patterns for kcu/epixhr rogue device instantiation?
  o (matt) how to ensure deadtime handled correctly
    o partitiondelay/triggerdelay/pausethreshold (trigdelay was 42
      and pausethreshold was 16 for epixhr)
  o running events
    - look at acceptframe here: ~/git/epix-100a-gen2/software/python/ePixViewer/_epixViewer.py
  o what registers are important (set to non-default vals) for our usage
  o data format (4 asic subframes or monolithic?): only 1 subframe
    o data shuffling? same as lcls1: likely not shuffled
  o how close is epix100 to epixhr? (config/data)
  o same version of submodules for kcu1500 and epix100
  o need setup.py to get submodules in conda
    o same version of submodules for kcu1500 and epix100
  o SlowAdcRegisters crashes gui
  o run gui with lcls2 timing and groupca

ami jenkins, mona, epics_pva_addr_list, procmgr fix
* managing sharing: readout groups, drp nodes, cnf files, timing system connections
 - Chris Ford is working on a shared-DAQ resource allocation manager
 - Matt Weaver has this big-optical-switch ("BOS") which is principle allows more flexible use of the shared DAQ nodes
 - right now we have rix/tmo "official" setups.  we should probably create another one for acr, and then another one for one-off setups (like what Bill Schlotter is doing tomorrow).
 - make drp nodes more uniform?
* per stream and per run event/dmg counts (caf)
** psana handling integrating detectors
* configure andor with configdb?
* fix lcls1 smalldata mpi.sum cores not having any events (cpo)
* support epix100
** benchmark SZ and other reduction for real expt data (mikhail)
  - can SZ handle gain-range detectors?
* put .cnf.running and .cnf.last in same dir as .cnf (caf)
* make it harder to leave out timing system
* broadcasting non-epics slowupdate data (e.g. ttool bkdg)
* move encoder to tcp (caf)
* crystfel to psana geom (mikhail)
* understand why atm opal det.raw.image() doesn't work with fex enabled (matt, cpo)
* move TMO to xpm:2 (like RIX)
* reduction algorithms
* turning low rate data into per-shot high rate (e.g. ttool background) ami+psana (mona)
* procmgr with hundreds of nodes (caf)
* explore object stores for psana like ceph (mona)
* alias not in xtc file so more flexible
* psana datasource for drp (mona)
* dump out all lcls2 config objects with psana (ro)
* fim link to xpm not locking (matt)
* teb having memory allocation problem after too many deallocate's (ric)
* select monitoring destination for particular events (ric)
* avoid missing 1 second of epicsarch data at beginning of run
* small daq teststand for automated continuous-integration tests?
* multisegment epicsinfo in pvadetector
* ami controlling DRP ROI via configdb? (seshu)
* twocanary (toucan) in srcf
* code/results migration to/from nersc/sdf
* psmon only update one of multiplot (ddamiani)
* psana test to verify that all detector interfaces follow the rules
* psmon XYPlot format list of one item breaks multiple lines silently (ddamiani)
* more psmon examples for andy (ddamiani)
* consider adding "safe" Xtc::alloc with max-size argument to avoid fixed-buf overruns (or maybe in higher layer)
* off-by-one enabling automation (stoppers)
* xface for ami controlling detector params
* web display of ami plots
* mhz bld (matt)
* hsdioc needing restart (matt)
* uniform (epics?) interface to detector config registers for config scans
* automatically push releases for lcls1/lcls2 to nersc/sdf/other
* hsd test pattern mode (matt)
* makepeds/calibrun regression tests and simplification or freeze? (mikhail)
* opals for different daq's on same machine (larry+pcds)
* fix psana1 test release LD_PRELOAD hack caused by removal of
  ' -Wl,--copy-dt-needed-entries -Wl,--enable-new-dtags'
  in SConsTools/psdm_cplusplus.py for gcc48.
* psana dbase access for live mode list of files
* handle bad external timing better? (matt)
* deadtime-per-detector in grafana
* generic detector calibration constants (e.g. manta)
* yves acremann detector
* teststand: kcu's to acc nodes (01,02,05,06), bring back ffb, fix ib manager and moving it to switch
* make swmr work with small-h5
** for phil: configdb_readxtc and compare configs (ro, caf)
* archon for rix (dan)
* improved psana1 tests (ro)
* lcls2 common-mode for panels in different gain ranges (mikhail)
* update rogue
* plims for lcls2 (mikhail)
* mpi jobs not exiting if one core crashes
* managing prometheus files in promdir
* put procstat in prometheus (or hard to duplicate all functionality?)
* how to deal with pickle in the calibdb? (mikhail)
* epicsarch support for strings (ric says it's hard)
* support damage in psana
* protect against use of keywords as epics aliases, consider using namespacing. write a sanitizer for epicsarch files (use "import keywords"?)
* send clearreadout to pvadetectors
* support uniform 3D arrays in areadetector interface
* remove git passwords for relmanage
* eliminate opal lcls1->lcls2 timing toggling (larry, ben)
* multiseg epics (kwargs or collection segids?) (mona)
* jungfrau/epix dark shot pedestals
* psana1: implement idx using smd
* correct setting for hyperthreading? (ric says important for MHz)
* psana2 idx mode
* psana1: add epix firmware id in epix id's for phil
* upgrade jupyterhub or use on-demand?
* general test that all public detector attributes are method taking event with return type (ro)
** support multiple cameras on one drp node
** one exe writing multiple files to improve performance
* move batch tests to slurm
** xtcdata duplicate names throw
* move away from afs (mv pdsdata/psalg repo)
* spares
** make everything work with Debug instead of RelWithDebInfo (valerio, ric)
* move readout group config info from segment levels to ts
* optionally don't record selected detectors to disk
* mikhail: roentdek with quadanode
* fix camlink converter box with opal where only one strip works (requires camlink powercycle)
* daq window placement
* parallelized calibrations (mikhail)
* state machine confusing (ric email) lower priority?
* monitor deadtime per-lane (matt)
* improved grafana (like ganglia)
* reduction algorithms (ric)
* move other hutches to lcls2
* allocation of readout groups and devices, lanes
** shared drp node allocation mechanism (caf)
* xpplr8116 geom calib const access for py3
* make hsd's more robust, e.g. timing dropouts (matt)
* LCLS2 drop shots (bykiks evtcode 161)
* MHz with real tmo analysis (mona)
* create more platforms for procmgr
* practice power outage
* parallel jupyterhub with visualization
* daq2 chunking (dan)
* portable gpu detector corrections (kokkos/hip/openmp/opencl?)
* deploy releases everywhere, containers? (valerio) (nersc, sdf, new/old psana1)
* unified/integrated timetool calibration
* fix failing new-style psana1 tests (valerio)
* shifter mounting permissions (mona, johannes)
* send multiple copy of events to shmem like lcls1
** small h5 ebeam/gasdet automatic storage (ro)
* small h5 ragged arrays
* meb (or later layer) broadcasts events to all clients (ami, python) (ric)
* read-while-write smallh5
* test daq sequences (caf)
* psana1 MTRX:V2 geometry (mikhail)
* update procServ (caf)
* slow updates need to obey deadtime (matt)
* setting msgdelay in timing system? (matt)
* syslog print throttling (caf)
* off-by-one (psana1/2) (ro)
* off-by-one support for low-rate devices
* units support in det xface
* put psana2 (and psana1?) on conda-forge
* calibdb dns issue (in travis macos build)
* replace travis
* in psana SRV callback only persist some fields to h5?
* once we have real data, work more on timetool calc in firmware
* dlopen for reduction alg
* scalable calib-fetch solution for shmem/drp
* sdf/nersc calib-dir sync (wilko)
* psana1: continuous integration of py3 (jenkins)
* lcls2 configdb tools: history (ro)
* algorithms (drp/ana, e.g. beam-center finding)
* singularity at slac
* support more python versions
* SDF support including ARP
* automatic users add to sdf lcls queue (wilko, valerio)
* python DRP? (ric, matt)
* fiber power readings everywhere (matt, tid)
* peaknet
* ami:
  - josh: nanosecond xpcs: 2 pulse acqiris.  ratio of peak areas, correlation
    export results to ACR
  - josh: xpcs photon counting, working with chuck: accumulate statistics
    and then fit, number of photons per shot in histogram (talk to chuck
    and silke sxrm23). nicholas burdet (shared postdoc)
* timetool (ben): tag to front end, fiber power, toggle xpmini->lcls2 timing, clear readout
* simpler interface for controlling teb/meb?
* xtcav daq recording epics variables as well (and also bld values?)
* xtcav/psocake py3 compatible

**********************************************************************

ray tracing:
few thousand rays
24 surfaces
few dozen configurations (max 200x200, angles/heights)

few thousand rays, 1 surface, 200 configurations: takes about 1 day

batch ray trace only stores results to one surface
max/min of few thousand floating point

**********************************************************************

1 rack of computing for ffb
 - 1000 cores 8 nodes in 4u
ffb from 0.5PB to 1 or 2
sdf storage 6 jbods 1.5PB in 1 jbod in 4u for 50k + $overhead
enlarge sdf compute ?
switching to new srcf-sdf in april

**********************************************************************

went through xpp/dxs so far:

increased best case pie-slice by 2x (float vs. int)
decreased ttool to 100kHz
hsd best case 10x for peaks?
agree with "most likely"
increased sparkpix worst and most-likely by 2x for pixel index info?
  - left best-case alone to give idea of large error bars
fixed dxs sparkpix 2M didn't respect the DR reduction factor column

fixed comment: no pie-slicing in cxi
cxi: fixed epix best-case DR factor from 0.25 to .05
fixed cxi sums (missing top two rows)

buhrmann cataract
referral from degrossier
saw buhrmann once before
https://www.herzig-eye.com/our-surgeons/ralf-buhrmann-mdcm-mph-phd-frcsc/
private eye institute
