* laser event codes (phase, xpm control, xpm-0 reboot), mtu IT jira, pulseid in psana, gmd/piranha correlation, scons test, mhz encoder, acr feedback (mcast/tcp), timing-in procedure, encoder infrastructure, lcls2 damage stats (and event counters?), cxix1000021:run5 det.image, integ events, acr feedback, alice green, txi kpp (grp7 grafana), psana1 live, rix integ det data, cmp041, 156 subnet jira, matt tdet firmware everywhere, more timing cables, epixM

kristjan requests:
- single-rate for (infinite sequence) rixgeneratory.py (one camera)
- add something for laser on/off with andors (infinite sequence)
  o every second, third, or fourth

det grp issues:
- may need epixhr at mfx (24 pairs, 20 data plus 4 timing)
  o only 6 pairs currently according to omar
- epixm at mfx: raw-data fibers 1 timing, 4 data, 1 register, others are
  edge-ML fibers and spares
- dan: rixs ccd in october
- wilko: asc data to s3df
  o can't go directly to s3df because of routing: go through ffb
  o can do it for dss nodes for lcls1 (slow becaue of gbit)
  o needs to check for lcls2
- matt epixhr running:
  - bld different timestamps:
    o wanted run trigger before the beam
    o insert extra run trigger event codes to emulate high rate (several time) after 140, so detector was triggered late, after the 140 (matt has diagram), so different timestamp
  - with bld unstable scans
  - with bld free running over 120Hz broke at 3am
    o matt doesn't know about that (maybe common-readout-group was no longer comon?).  need to have sequence with right parameters to get event code 280 (common readout group, and also the run trigger) to exactly match event code 140
  - complexity (e.g. running on mon001, gateway should take care of it)
  - mike browne:
    o ioc wasn't running. confusion about mpod. two ioc functions: check power
      supply, get detector temperatures.
    o thermistors from monitoring ioc, keithley
- ability to run at asc or mfx?
  - should be easy

matt projects:
switch to official BLD data-description server
schedule acr/rix accelerator burst-pattern mtg
hsd repair
tmo-to-acr feedback without gateways (bld or epics or both)
test all detectors in tmo/rix
robustness of timing links (install new tdet firmware everywhere?)
long ami 11s latency seen in both tmo/rix under some conditions
add sp1k4 gige to tmo.cnf
add mono04 gige to rix.cnf

ric projects:
- psana grafana
- acr mcast feedback
- libsz to roi switch (and easily change roi config params)
- epixhremu to 20 nodes
- multicamlink drp

riccardo proj:
- gmd/xgmd running in rix
- epixuhr
- finishing uproot
- batchtest in s3df (requires openmpi with pmi2, not pmix)
- juhao work
- reproduce timing problems in fee teststand

mona proj:
- epixM (get the detector from dionisio)
- more timing fibers for ric epixhr emulation
- high rate encoder
- rix piranha
- live mode sleep problem
- destination callback (conflicts with integrating detector?). work with silke,andy@txi
- moving vds h5?
- h5 time sort

s3df issues:
- silke on contact page?
- do preemptable jobs have a priority or first-come-first-serve?
- ampere folding processes with no batch jobs?
- seshu ampere jobs
- run clush with sudo? (need to ssh to sdfmgr002)
- hierarchy of shares (waiting on procurements, this week): will set to number of "purchased" cores: cryoem: 33, fermi 33, lcls 111, rubin 1?, ad 13, simes 1?
- how does slurm gather cores for mpi?
- "perazzo" sharing (needs a scalable way of calling the "delta" function)
  o delta-function (allocated vs. used) takes a long time to compute: can create a denial-of-service of slurm since it's slow
  o "forcing" people into preemptable when they exceed their allocation
- jupyter automatic queue selection
- weka: read-while-write (permissions fixed with 4.2.4)
- silke: coordinator roles deprecated for job kill, since it's global (rubin, lcls...)

tmo detectors:

Detectors to be ready in June: 2X FIMs (2 on KB1), ATM opal&piranha for IP1, FZP opal&piranha, HSDs, BLD, Laser wav8, IM5K4 

Detectors to be added by the end of calendar year 2023: two more FIMs on KB2 and IM6K4, ATM opal&piranha for IP2, another Laser wav8 for IP2

detector requests:
- (det) rixccd archon (installed in hutch march 2023, beam oct 2023?)
- superconducting BLD (fall 2022)
- (tid) high-rate mono-encoder
  o interpolated absolute encoder (march 2023)
  o high rate relative encoder (december 2023)
- dream hsd's in jan. 2023
- (unapproved) ikon (for SVLS, kristjan et al., early 2023, perhaps princeton mid-late 2023?)
- (tid, det?) k-microscope dld (test early summer 2022, production in early 2023?, beam summer 2023)
- ami roiarch with 3D (mikhail)
- (det) uxi needs more work from dan (lcls1)
- (standalone approved) (tid, det) tixel (tmo standalone rogue tmo, dec. 2022?, daq integration if looks good)
  o bojan in TID is doing the work
- (tid, det) epixM (lorenzo, integration prototype (no asic, no emulation) from tid beginning of end of august 2023 with asics? 2 320kpx, 5kHz),
  - no fixed gain mode, only one autorange mode AHL, some gain ambiguity that needs to be resolved
  - integrate in november 2022.  full cam in june 2023.
  - MTP24 broken out to 3 MTP8 fiber 6.4GB/s
  - 384*192*2 per camera (737MB/s per camera @5kHz)
- (tid,det) epixHR (prototype quad in march, start work sept 2022, 2Mpx, 5kHz, summer 2023) single MTP24 broken out to 3 MTP8 fibers: prototype in august 2022, beam in sept 2022 (and dec 22). quad in November 2022.  full cam in march 2023.  full 2M det commissioning june 2023.  quad is different than single.
  - 5 to 10 fibers pairs needs to be converted (use local xpm to generate
    multimode).  maybe 12 if edge-ml stuff.
  - 2 small epixHR's (140kpx?) single MTP12
- cookiebox (early 2024): guarantee 8 channels, hope for 16
- (will approve) (tid) opal replacement (s991? photometrics kinetix? giacomo thinks perhaps alvium?) https://www.alliedvision.com/en/products/alvium-configurator/alvium-1800-u/240/#_configurator
- (approved for standalone) (tid,det) timepix for rix: standalone mode in July 2022 (256x256 px,
  o get start-time and time-over-threshold, ~2kHz, could be faster)
  o working with anton tremsin at berkeley
- (unapproved) (det) varex xrd 4343 cameras for mec? (offered assistance, lcls1, needed april/may 2023)
- (unapproved) orca quest hamamatsu cxp camera? (lcls1, xpp? matt seaberg takahiro)
  https://www.hamamatsu.com/eu/en/product/cameras/qcmos-cameras/C15550-20UP.html
- (tid, det) small epixUHR (200x200): (emulator available end of feb. 2023) beam-test with daq april 2023 (35kHz).
  not identical to epixHR.
  - daq integration with firmware emulator in jan 2023?  prototype 1 asic in april 2023? beam time soon after
- andor iXon for UED (flexible, June/July 2023?)
- axis-sxr-60-std 36MPx 26Hz camera request from Kristjan in RIX
  https://www.axis-photon.com/streak-camera/axis-sxr-60-36mpix-soft-x-ray-scmos-camera/

* share simple python code between ami2/psana1/psana2 (amityping, psmon, "HPolar")
* datasource kwarg to suppress fetching of calib constants for mikhail's mask editor (mona)
* split daq/ana conda envs (valerio)
* calibconst for lcls1 for ami2 (e.g. jungfrau)
* saxs/waxs data reduction (stefano)
* uniform (epics?) interface to detector config registers for config scans
* fluctuation saxs/waxs data reduction (stefano)
* manage running-condition dependent calibration constants (e.g. rate dependent) (tweak serial id?) (mikhail)
* improve destination callback (mona)
* event counts in logbook for phil
* monitoring new serial numbers for LCLS2 (mikhail)
* release DMA buffers earlier? currently released at same time as pebble (ric)
* small VDS "link files" seem to have absolute path to partN files. should be relative path.
* running daq with missing bld (ric claus)
* running daq with damaging pvadet (ric claus)
* fix daq timeout behavior where we sometimes get damage from many nodes  (ric claus)
* multi camlink cameras in a node (ric claus)
* converting grafana values into epics alarms
* configuration management/control for calibration/fexA/fexB
psana conflict between normalized integrating detectors and cube with destination callback? perhaps not a problem since integrating detector requires stable pump-probe time
* sphinx-like (read-the-docs?, makedocs, github?, confluence) auto-generated documentation (zach, ken, tom caswell) (mikhail)
* eliminate psana1 calibdir in favor of database (mikhail)
* eliminate intermediate "panel" files on disk and put in database (for epix10k/jungfrau) (mikhail)
* didn't get obvious error when writing to unwriteable wave8 pv register (MasterEnable) (riccardo, mona, ric)
* event counts in elog for phil. jira ecs-3843 (chris ford, mikhail)
* calibration constant provenance (mikhail)
* h5 full-pathname issue in part0 files
* resurrect psana1 unit tests ("scons test") (mikhail)
* ami python editor returning array with one element into ScalarPlot generates no visible error (but see error in logs)
* when rix piranha had small number of dma buffers (like opal) we often, but not always, got 100% deadtime and BEB timeouts.  feels like buffers not configurerd correctly when deadtime happens (ric, matt, mona, riccardo)
* force inclusion of all necessary TREX data for xtcav
* utility to put lcls2 h5 files in time-order (mona? mikhail? caf? riccardo?)
* understand/fix "384" intermittent libfabric ib completion queue issue (ric) (currently "fixed" by running eblf_pingpong)
* only 4 bits for xpm number in xpm remote link id (matt)
* more hsd's for simultaneous cookiebox/dream running (14+16 channels)
  but maybe not more hsd's if tixel works (weaver)
* not all errors show up in ami gui, e.g. missing attribute error (ddamiani)
* andor pedestal subtraction (mikhail)
* managing sharing: readout groups, drp nodes, cnf files, bos connections
 - Chris Ford is working on a shared-DAQ resource allocation manager
 - Matt Weaver has this big-optical-switch ("BOS") which is principle allows more flexible use of the shared DAQ nodes
 - right now we have rix/tmo "official" setups.  we should probably create another one for acr, and then another one for one-off setups (like what Bill Schlotter is doing tomorrow).
 - make drp nodes more uniform?
* mask editor (mikhail)
* when detector list is long in control-gui, can't hit "apply" button  (mikhail)
* when running ami in daq detector list is intermittently not updated (ddamiani)
* have daq bluesky scan scripts default to "scan" as scan detector name to avoid psana crash (caf)
* ability for select-gui to allow teb's to be deselected (down to 1) (mikhail)
* epix100/epixhr deadtime not working (10Hz with epix100)
* ami 3D angular integration (mikhail)
* per stream and per run event/dmg counts (caf)
* better management of drp .service files (shared drive, puppet, kubernetes?)
** integrating detector "exposure time" psana param (mona)
* configure andor with configdb?
* silver behenate fitting (including z-stagger) for panel positions (mikhail)
** benchmark SZ and other reduction for real expt data (mikhail)
  - can SZ handle gain-range detectors?
* make it harder to leave out timing system
* algorithm for non-quantized det-image charge sharing (mikhail)
* raise exception instead of crashing when idx/smd files are missing in psana1
* broadcasting non-epics slowupdate data (e.g. ttool bkdg)
* control_gui not showing up on right screen in tmo (ami, procstat do) (mikhail)
* test psana2 live mode (mona)
* move encoder to tcp (caf)
* crystfel to psana geom (mikhail)
* ami mypy daemon is long-lived: can cross release boundaries
* configdb management for changing fex/teb decisions
* run lcls1 jungfrau at 120Hz (ddamiani, dan)
* timetool slowupdate background handling in psana (mona)
* procmgr performance with hundreds of nodes (caf)
* explore object stores for psana like ceph (mona)
* alias not in xtc file so more flexible
* select monitoring destination for particular events (ric)
* avoid missing 1 second of epicsarch data at beginning of run
* small daq teststand for automated continuous-integration tests?
* multisegment epicsinfo in pvadetector
* ami controlling DRP general ROI with masks via configdb? (mikhail)
* psmon only update one of multiplot (ddamiani)
* psana test to verify that all detector interfaces follow the rules
* psmon XYPlot format list of one item breaks multiple lines silently (ddamiani)
* more psmon examples for andy (ddamiani)
* off-by-one enabling automation (stoppers)
* xface for ami controlling detector params
* web display of ami plots
* mhz bld (matt)
* bld data structure populated automatically from epics
* automatically push releases for lcls1/lcls2 to nersc/sdf/other
* makepeds/calibrun regression tests and simplification or freeze? (mikhail)
* opals for different daq's on same machine (larry+pcds)
* fix psana1 test release LD_PRELOAD hack caused by removal of
  ' -Wl,--copy-dt-needed-entries -Wl,--enable-new-dtags'
  in SConsTools/psdm_cplusplus.py for gcc48.
* psana dbase access for live mode list of files
* handle bad external timing better? (matt)
* generic detector calibration constants (e.g. manta)
* teststand: kcu's to acc nodes (01,02,05,06), bring back ffb, fix ib manager and moving it to switch
* make swmr work with small-h5
** for phil: configdb_readxtc and compare configs (ro, caf)
* lcls2 common-mode for panels in different gain ranges (mikhail)
* plims for lcls2 (mikhail)
* mpi jobs not exiting if one core crashes
* address psana/daq prometheus issues described on confluence (mona/ric)
* managing prometheus files in promdir
* put procstat in prometheus (or hard to duplicate all functionality?)
* how to deal with pickle in the calibdb? (mikhail)
* epicsarch support for strings (ric says it's hard)
* support damage in psana, including counting (mona)
* protect against use of keywords as epics aliases, consider using namespacing. write a sanitizer for epicsarch files (use "import keywords"?)
* send clearreadout to pvadetectors
* eliminate opal lcls1->lcls2 timing toggling (larry, ben)
* multiseg epics (kwargs or collection segids?) (mona)
* jungfrau/epix dark shot pedestals
* psana1: implement idx using smd
* correct setting for hyperthreading? (ric says important for MHz)
* psana2 idx mode
* psana1: add epix firmware id in epix id's for phil
* upgrade jupyterhub or use on-demand?
* general test that all public detector attributes are method taking event with return type (ro)
** support multiple cameras on one drp node
** one exe writing multiple files to improve performance
** xtcdata duplicate names throw
* move away from afs (mv pdsdata/psalg repo)
* spares
** make everything work with Debug instead of RelWithDebInfo (valerio, ric)
* move readout group config info from segment levels to ts
** optionally don't record selected detectors to disk
* mikhail: roentdek with quadanode
* fix camlink converter box with opal where only one strip works (requires camlink powercycle)
* daq window auto placement
* parallelized calibrations (mikhail)
* state machine confusing (ric email) lower priority?
* monitor deadtime per-lane (matt)
* improved grafana (like ganglia)
* move other hutches to lcls2
* allocation of readout groups and devices, lanes
* LCLS2 drop shots (bykiks evtcode 161)
* MHz with real tmo analysis including pre/post processing partitioning (mona)
* create more platforms for procmgr, perhaps with offsets for each hutch based on xpm? (caf)
* procmgr don't complain about undefined platform when all ports are specified (e.g. neh-base.cnf)
* practice power outage
* parallel jupyterhub with visualization (copy euxfel?) (riccardo/mona/wilko/valerio)
* portable gpu detector corrections (kokkos/hip/openmp/opencl?)
* deploy releases everywhere, containers? (valerio) (nersc, sdf, new/old psana1)
* unified/integrated timetool calibration
* fix failing new-style psana1 tests (valerio)
* shifter mounting permissions (mona, johannes)
* send multiple copy of events to shmem like lcls1
** small h5 ebeam/gasdet automatic storage (ro)
* small h5 ragged arrays
* meb (or later layer) broadcasts events to all clients (ami, python) (ric)
* read-while-write smallh5
* test daq sequences (caf)
* psana1 MTRX:V2 geometry (mikhail)
* update procServ (caf)
* slow updates need to obey deadtime (matt)
* setting msgdelay in timing system? (matt)
** syslog print throttling (caf)
* off-by-one (psana1/2) (ro)
* off-by-one support for low-rate devices
* units support in det xface
* put psana2 (and psana1?) on conda-forge
* calibdb dns issue (in travis macos build)
* in psana SRV callback only persist some fields to h5?
* once we have real data, work more on timetool calc in firmware
* dlopen for reduction alg
* scalable calib-fetch solution for shmem/drp
* sdf/nersc calib-dir sync (wilko)
* psana1: continuous integration of py3 (jenkins)
* lcls2 configdb tools: history, delete (into "trash" folder for recovery?) (ro)
* algorithms (drp/ana, e.g. beam-center finding)
* singularity at slac
* support more python versions
* S3DF support including ARP
* automatic users add to sdf lcls queue (wilko, valerio)
* python DRP? (valerio, ric)
* fiber power readings from timing system kcu's (matt)
* peaknet
* ami:
  - josh: nanosecond xpcs: 2 pulse acqiris.  ratio of peak areas, correlation
    export results to ACR
  - josh: xpcs photon counting, working with chuck: accumulate statistics
    and then fit, number of photons per shot in histogram (talk to chuck
    and silke sxrm23). nicholas burdet (shared postdoc)
* timetool (ben): tag to front end, fiber power, toggle xpmini->lcls2 timing, clear readout
* simpler interface for controlling teb/meb?
* xtcav daq recording epics variables as well (and also bld values?)
* xtcav/psocake py3 compatible

**********************************************************************

peppex cabling:
first cassette count from 1;
slot 2 hsd timing
slot 3 hsd leftmost
slot 4 hsd second from left

second cassette count from 1:
slot 4 hsd second from right
slot 5 hsd right
slot 6 opal

hsd timing plugged into xpm4 amc0 port 3 counting from 0

**********************************************************************

txi hxr kpp measurements: (aquila)
https://confluence.slac.stanford.edu/pages/viewpage.action?spaceKey=L2SI&title=RP+testing
- scheduled around halloween '23?
- 1kHz and 10kHz SC beam going through hxr undulators into txi
- wave8 daq (synced trigger)
- want 100Hz old hxr gasdet bld, gate width up to andy/philheimann
  o will do with unsynchronized epics.  silke says someone is working
    on this (called "GEM")
- NO ebeam bld (hxr bld (e.g. gasdet, some ebeam)  will be 120Hz?)
- epics
- no daq camera. use untimestamped controls camera, perhaps recorded in daq.
- need single mode fibers, may need cassettes
- timing in wave8 will be done earlier in tmo
- just wave8 and a bunch of epics variables
- other epics: power meter (andy, can get name from jyoti) controls uv sensitive diodes
  (alyssa, jyoti can provide epics names).  get gem-gdet names from silke (she
  said it might be working on may 11, 2023)

tickets:

https://jira.slac.stanford.edu/browse/LCLSECSD-1546
https://jira.slac.stanford.edu/browse/LCLSECSD-1874

**********************************************************************

lab3: 68.2F later: 66.8F
pietro's lab: 69.5F later: 69.5F
fee alcove outside racks: 71.3F
fee alcove inside rack: 81.4F

lab3 nodes:
drp-tst-dev008, dev004, acc05, acc02, acc01,
daq-tst-dev02 (add kcu?), dev03 (add kcu?), dev06 (hsd)

**********************************************************************

epixhremu timing link status

34: bad rxclock:  no fiber
37: looked good (linkup) -3.49
38: bad rxclock -3.69 (fixed by plugging/unplugging)
39: down: no fiber
40: rxclock 0: -26.22
41: bad rxclock: -3.9
42: bad rxclock: -3.6 (fixed by plugging/unplugging)

**********************************************************************

rix device issues:
piranha 71kHz and 100dt at 10kHz
mr2k1: no timing sub-window (just "no EVR" message)
hsd_0,1 (and dt with hsd2,3 eventually)
encoder
only chemrix wave8
manta not downstream of xpm?

recorded rixx1003721 run 169 with
71kHz: timing, wave8
10Hz: opal,manta
1Hz: vls,norm,dir

recorded rixx1003721 run 171 with
71kHz: timing, wave8, hsd_0, hsd_1
10Hz: opal,manta,vls,norm,dir,mr2k1(untimestamped),exsmanta

norm/dir rog 5, evtcode 272 (1Hz)
vls,opal,manta rog 6, evtcode 276 (10Hz, gated)
timing,wave8 rog 2, evtcode 284 (33kHz, gated)

c**********************************************************************

rix doesn't see problems at the moment. so maybe xpm0 is happy?

shortterm:

1. clean fibers and change transceivers between xpm0 and xpm2
2. separate xpm's in the atca crate
3. eject xpm2/xpm4 (should be no difference with fru_deactivate)
   (less likely since 6,4 go down) change transceivers
   (less likely since 6,4 go down) clean fibers
4. remove xpm2 (xpm0->xpm4->xpm6) (currently 6 is driven by 2)
5. swap xpm's with fee alcove
6. new version of software broke it? (unlikely since rix is working)
7. more RF isolation with air resistance modules
8. pyxpm software overwriting registers
9. only xpm's with front-panel inputs have trouble? (or "second tier leaf")

xpm4,5,6: noRTM
xpm0,2,3: RTM

longterm:

- reproduce in fee alcove (by unplugging fiber from ACR)
- instrument firmware
  o PLL input frequency drifting out of range (lock status)
  o power glitching
- is it a lingering temperature effect?  (maybe reproducible in the
  teststand)

possible partial solutions:
- reset state machine that gets stuck (we kludged a fix once with a loopback)
  Matt has an idea here.

timing issues:
- spares (including rtm's)
- txlinkreset (firmware)
- xpm link glitches (firmware)
- toggle between xpmmini/lcls2
- 25kHz fiducial
- xpm ports won't lock to themselves
- no link lock on timing KCUs
- distribute knowledge more widely
- general diagnostic techniques for understanding problems like the above (e.g. how do we know a PLL input is within a required frequency range?)
- want to try: synchronous retransmission (no clock domain crossing)
  o can be done without trying to understand firmware
- not just them telling us what to do: deeper committment
  o needs gert

**********************************************************************

cov normalized to stddev of gmd np.cov(30 events)/np.std(gmd (30 events)) threshold and sum

fzp trace: threshold and sum over pixels (1 number) per event
gmd: 1 number per event, gmd.perpulseenergy

record 100 shots (two arrays of 100 numbers)
np.cov(fzp,gmd)/np.mean(gmd)

ami (pythoneditor?) script

trigger rate 100Hz (1MHz+destination 4 (mask 0x10))

friday night, or saturday night
run without intensifier (time separately)

for ATM: time the laser to the x-ray, then time the cameras to the laser
saturday night: timed in laser on the diode
monday/tuesday afternoon: time cameras to the laser
acr feedback: not needed until xtcav (few weeks)

took tmoc00221 run 38 as a test scan with fzp/atm piranhas (atm piranha is a standin for gmd)
