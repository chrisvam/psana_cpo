* xtcav, tqdm, vincent, fee teststand, ami2 ci, tpr driver, workshop video, slow mmu, slowupdate, pytorch, camlink timing workaround, piranha fex

- global: timing glitches, move atca to mon001 (and permanent netconfig), running neh-base.cnf from 4.5.17, piranha background (jan.) (slowupdate and L1)
- tmo: hsd's, peppex hsd's/opal, acr feedback test
- rix: no ami grafana
- lower priority: fibers to clean up

high rate commissioning:
- check l0delays and round-trip times and buffering

srcf to do:
- atm edge-finding for piranha and feedback
- daq kafka bus all-files-started
- buffering settings (test SIGSUSPEND for each device)
*- monitoring decision for low-rate dets
- protect against hsd firmware unhappy if too many fex peaks (matt)
* psana damage handling
- remember detector readout groups in cnf? (done by caf)
- epicsarch not writing out 1MHz l1accepts (or goes in slow readout group)
- xpm fanouts in srcf (maybe not urgent since only piranha's need them?)
*- auto coupled deadtime xpm register support in timing system python
- time in detectors
*- need slowupdate to obey deadtime? (messes up Ric's control-Z test of deadtime in srcf)
*- move all tmo devices to xpm:2
- move the pyxpm processes/network to SRCF
  - could also have local connections from e.g. rix nodes to the rix atca crate
  - short term: DAQ:NEH:XPM/DAQ:TMO:HSD/RIX:FIM:W8/MR4K2:FIM:W8/MR3K4:FIM:W8/MR2K4:FIM:W8
    goes in the gateway RW
- ami-settable data reduction params (e.g. thresholds for hsd)
*- scale to many drp's
- high rate dets
  - piranha
*  - bld
  - hsd
  - wave8
  - epixhr
- low rate det
  - epicsarch
  - pvadet
- roentdek alg (done by mona)
- chunking (done by caf)
*- python drp
- rixccd
- psana
  - live-mode
*- dynamic resource mgmt
*- automatic off by one detection

detector requests:
- (det) rixccd archon (working in daq sep 2022, beam 2 months later)
- superconducting BLD (fall 2022)
- (tid) high-rate mono-encoder
  o interpolated absolute encoder (march 2023)
  o high rate relative encoder (december 2023)
- dream hsd's in jan. 2023
- (unapproved) ikon (for SVLS, kristjan et al., early 2023, perhaps princeton mid-late 2023?)
- (tid, det?) k-microscope dld (test early summer 2022, production in early 2023?, beam summer 2023)
- (det) uxi needs more work from dan (lcls1)
- (standalone approved) (tid, det) tixel (tmo standalone rogue tmo, dec. 2022?, daq integration if looks good)
  o bojan in TID is doing the work
- (tid, det) epixM (start work nov 2022, 2 320kpx, 5kHz, commission april/june 2023),
  - no fixed gain mode, only one autorange mode AHL, some gain ambiguity that needs to be resolved
  - integrate in november 2022.  full cam in june 2023.
  - MTP24 broken out to 3 MTP8 fiber 6.4GB/s
  - 384*192*2 per camera (737MB/s per camera @5kHz)
- (tid,det) epixHR (start work sept 2022, 2Mpx, 5kHz, summer 2023) single MTP24 broken out to 3 MTP8 fibers: prototype in august 2022, beam in sept 2022 (and dec 22). quad in November 2022.  full cam in march 2023.  full 2M det commissioning june 2023.  quad is different than single.
  - 5 to 10 fibers pairs needs to be converted (use local xpm to generate
    multimode).  maybe 12 if edge-ml stuff.
  - 2 small epixHR's (140kpx?) single MTP12
- (will approve) (tid) opal replacement (s991? photometrics kinetix? giacomo thinks perhaps alvium?) https://www.alliedvision.com/en/products/alvium-configurator/alvium-1800-u/240/#_configurator
- (approved for standalone) (tid,det) timepix for rix: standalone mode in July 2022 (256x256 px,
  o get start-time and time-over-threshold, ~2kHz, could be faster)
  o working with anton tremsin at berkeley
- (unapproved) (det) varex xrd 4343 cameras for mec? (offered assistance, lcls1, needed april/may 2023)
- (unapproved) orca quest hamamatsu cxp camera? (lcls1, xpp? matt seaberg takahiro)
  https://www.hamamatsu.com/eu/en/product/cameras/qcmos-cameras/C15550-20UP.html
- (tid, det) small epixUHR (200x200): beam-test with daq april 2023 (35kHz).
  not identical to epixHR.
  - daq integration with firmware emulator in jan 2023?  prototype 1 asic in april 2023? beam time soon after
- andor iXon for UED (flexible, June/July 2023?)

* manage running-condition dependent calibration constants (e.g. rate dependent) (tweak serial id?) (mikhail)
* improve destination callback (mona)
* monitoring new serial numbers for LCLS2 (mikhail)
* small VDS "link files" seem to have absolute path to partN files. should be relative path.
* swapping slurm jobs script (ro)
* update aes-stream-driver to 64-bit so we can unpin rogue
* eliminate intermediate "panel" files on disk and put in database (for epix10k/jungfrau) (mikhail)
* exclude-detector list (mona)
* calibration constant provenance (mikhail)
* h5 full-pathname issue in part0 files
* ami python editor returning array with one element into ScalarPlot generates no visible error (but see error in logs)
* understand/fix "384" intermittent libfabric ib completion queue issue (ric) (currently "fixed" by running eblf_pingpong)
* long eb connect times in rix (ric)
* andor pedestal subtraction (mikhail)
* managing sharing: readout groups, drp nodes, cnf files, timing system connections
 - Chris Ford is working on a shared-DAQ resource allocation manager
 - Matt Weaver has this big-optical-switch ("BOS") which is principle allows more flexible use of the shared DAQ nodes
 - right now we have rix/tmo "official" setups.  we should probably create another one for acr, and then another one for one-off setups (like what Bill Schlotter is doing tomorrow).
 - make drp nodes more uniform?
* mask editor (mikhail)
* ability for select-gui to allow teb's to be deselected (down to 1) (mikhail)
* epix100/epixhr deadtime not working (10Hz with epix100)
* ami angular integration
* per stream and per run event/dmg counts (caf)
** integrating detectors (daq and psana)
* configure andor with configdb?
** benchmark SZ and other reduction for real expt data (mikhail)
  - can SZ handle gain-range detectors?
* make it harder to leave out timing system
* algorithm for non-quantized det-image charge sharing (mikhail)
* broadcasting non-epics slowupdate data (e.g. ttool bkdg)
* create an example of parallel smallh5 with mpi/dask (riccardo)
* test psana2 live mode (mona)
* move encoder to tcp (caf)
* crystfel to psana geom (mikhail)
* ami mypy daemon is long-lived: can cross release boundaries
* configdb management for changing fex/teb decisions
* run lcls1 jungfrau at 120Hz (ddamiani, dan)
* move TMO to xpm:2 (like RIX)
* reduction algorithms
* turning low rate data into per-shot high rate (e.g. ttool background, encoder) ami+psana (mona)
* timetool slowupdate background handling in psana (mona)
* elegant early shutdown of psana with smd0 irecv for mtip? (mona)
* ami2 array thresholding (ro)
* procmgr performance with hundreds of nodes (caf)
* explore object stores for psana like ceph (mona)
* alias not in xtc file so more flexible
* psana datasource for drp (valerio/mona)
* teb having memory allocation problem after too many deallocate's (ric)
* select monitoring destination for particular events (ric)
* avoid missing 1 second of epicsarch data at beginning of run
* small daq teststand for automated continuous-integration tests?
* multisegment epicsinfo in pvadetector
* ami controlling DRP ROI via configdb? (seshu)
* twocanary (toucan) in srcf
* code/results migration to/from nersc/sdf
* psmon only update one of multiplot (ddamiani)
* psana test to verify that all detector interfaces follow the rules
* psmon XYPlot format list of one item breaks multiple lines silently (ddamiani)
* more psmon examples for andy (ddamiani)
* consider adding "safe" Xtc::alloc with max-size argument to avoid fixed-buf overruns (or maybe in higher layer)
* off-by-one enabling automation (stoppers)
* xface for ami controlling detector params
* web display of ami plots
* mhz bld (matt)
* uniform (epics?) interface to detector config registers for config scans
* automatically push releases for lcls1/lcls2 to nersc/sdf/other
* hsd test pattern mode (matt)
* makepeds/calibrun regression tests and simplification or freeze? (mikhail)
* opals for different daq's on same machine (larry+pcds)
* fix psana1 test release LD_PRELOAD hack caused by removal of
  ' -Wl,--copy-dt-needed-entries -Wl,--enable-new-dtags'
  in SConsTools/psdm_cplusplus.py for gcc48.
* psana dbase access for live mode list of files
* handle bad external timing better? (matt)
* deadtime-per-detector in grafana
* generic detector calibration constants (e.g. manta)
* teststand: kcu's to acc nodes (01,02,05,06), bring back ffb, fix ib manager and moving it to switch
* make swmr work with small-h5
** for phil: configdb_readxtc and compare configs (ro, caf)
* archon for rix (dan)
* improved psana1 tests (ro)
* lcls2 common-mode for panels in different gain ranges (mikhail)
* plims for lcls2 (mikhail)
* mpi jobs not exiting if one core crashes
* managing prometheus files in promdir
* put procstat in prometheus (or hard to duplicate all functionality?)
* how to deal with pickle in the calibdb? (mikhail)
* epicsarch support for strings (ric says it's hard)
* support damage in psana (including counting)
* protect against use of keywords as epics aliases, consider using namespacing. write a sanitizer for epicsarch files (use "import keywords"?)
* send clearreadout to pvadetectors
* support uniform 3D arrays in areadetector interface
* remove git passwords for relmanage
* eliminate opal lcls1->lcls2 timing toggling (larry, ben)
* multiseg epics (kwargs or collection segids?) (mona)
* jungfrau/epix dark shot pedestals
* psana1: implement idx using smd
* correct setting for hyperthreading? (ric says important for MHz)
* psana2 idx mode
* psana1: add epix firmware id in epix id's for phil
* upgrade jupyterhub or use on-demand?
* general test that all public detector attributes are method taking event with return type (ro)
** support multiple cameras on one drp node
** one exe writing multiple files to improve performance
** xtcdata duplicate names throw
* move away from afs (mv pdsdata/psalg repo)
* spares
** make everything work with Debug instead of RelWithDebInfo (valerio, ric)
* move readout group config info from segment levels to ts
* optionally don't record selected detectors to disk
* mikhail: roentdek with quadanode
* fix camlink converter box with opal where only one strip works (requires camlink powercycle)
* daq window placement
* parallelized calibrations (mikhail)
* state machine confusing (ric email) lower priority?
* monitor deadtime per-lane (matt)
* improved grafana (like ganglia)
* move other hutches to lcls2
* allocation of readout groups and devices, lanes
** shared drp node allocation mechanism (caf)
* LCLS2 drop shots (bykiks evtcode 161)
* MHz with real tmo analysis including pre/post processing partitioning (mona)
* create more platforms for procmgr, perhaps with offsets for each hutch based on xpm? (caf)
* procmgr don't complain about undefined platform when all ports are specified (e.g. neh-base.cnf)
* practice power outage
* parallel jupyterhub with visualization (copy euxfel?) (riccardo/mona/wilko/valerio)
* portable gpu detector corrections (kokkos/hip/openmp/opencl?)
* deploy releases everywhere, containers? (valerio) (nersc, sdf, new/old psana1)
* unified/integrated timetool calibration
* fix failing new-style psana1 tests (valerio)
* shifter mounting permissions (mona, johannes)
* send multiple copy of events to shmem like lcls1
** small h5 ebeam/gasdet automatic storage (ro)
* small h5 ragged arrays
* meb (or later layer) broadcasts events to all clients (ami, python) (ric)
* read-while-write smallh5
* test daq sequences (caf)
* psana1 MTRX:V2 geometry (mikhail)
* update procServ (caf)
* slow updates need to obey deadtime (matt)
* setting msgdelay in timing system? (matt)
* syslog print throttling (caf)
* off-by-one (psana1/2) (ro)
* off-by-one support for low-rate devices
* units support in det xface
* put psana2 (and psana1?) on conda-forge
* calibdb dns issue (in travis macos build)
* in psana SRV callback only persist some fields to h5?
* once we have real data, work more on timetool calc in firmware
* dlopen for reduction alg
* scalable calib-fetch solution for shmem/drp
* sdf/nersc calib-dir sync (wilko)
* psana1: continuous integration of py3 (jenkins)
* lcls2 configdb tools: history, delete (into "trash" folder for recovery?) (ro)
* algorithms (drp/ana, e.g. beam-center finding)
* singularity at slac
* support more python versions
* S3DF support including ARP
* automatic users add to sdf lcls queue (wilko, valerio)
* python DRP? (valerio, ric)
* fiber power readings everywhere (matt, tid)
* peaknet
* ami:
  - josh: nanosecond xpcs: 2 pulse acqiris.  ratio of peak areas, correlation
    export results to ACR
  - josh: xpcs photon counting, working with chuck: accumulate statistics
    and then fit, number of photons per shot in histogram (talk to chuck
    and silke sxrm23). nicholas burdet (shared postdoc)
* timetool (ben): tag to front end, fiber power, toggle xpmini->lcls2 timing, clear readout
* simpler interface for controlling teb/meb?
* xtcav daq recording epics variables as well (and also bld values?)
* xtcav/psocake py3 compatible

**********************************************************************

1 rack of computing for ffb
 - 1000 cores 8 nodes in 4u
ffb from 0.5PB to 1 or 2
sdf storage 6 jbods 1.5PB in 1 jbod in 4u for 50k + $overhead
enlarge sdf compute ?
switching to new srcf-sdf in april

**********************************************************************

2 filesystems: ffb (700TB) and offline (12PB)
with bigger disks can move ffb from 700TB to 1.4PB
700TB = 10 hours at 20GB/s

gpu:
2milan 32 high clock rate
4 nvidia a100

1 box 4 servers 2x64 core 1 box 512 cores 5K cores total

**********************************************************************

peppex cabling:
first cassette count from 1;
slot 2 hsd timing
slot 3 hsd leftmost
slot 4 hsd second from left

second cassette count from 1:
slot 4 hsd second from right
slot 5 hsd right
slot 6 opal

hsd timing plugged into xpm4 amc0 port 3 counting from 0

**********************************************************************

timepix discussion 06/06/22:

can use external trigger or free-running: we will use EVR-generated 120Hz trigger
main physics question to answer: can we separate one photon from two
will run at 120Hz
to synchronize with DAQ data can send extra ttl reset on a "slow" DAQ sequence to reset FPGA counter: this is more complex so don't do this
BNC or SMA with 50ohms
timing-in is easy: can use long exposure times and measure the time of the signal
jyoti will set up remote desktop to access anton's windows machine in the hutch via ICS

integrating comm protocol:
- timepix3 is event-driver (current one is timepix1)
- integrate timepix3
- use their fpga
- send jumbo udp packets on fiber. each photon is 6 bytes.
- suggests using their fpga timestamp

**********************************************************************

4000*2300*2*120=2GB/s (100Gbit enet)
triggering over cxp
fiber or bnc readout? (quad cxp6)
many other detectors (8 definite, 2 possible, varex)
lcls2: front-end box with timing and quad cxp6 in, pgp out
software timestamping for lcls1?  lcls2: firmware

**********************************************************************

rename roiarch?
masking and normalization (e.g. epix gaps)
arch outside image

- masks (define in ami?)
- 2d, 3d image
- q option with more parameters
- inherited/coupled parameters for multiple roiarch's

broken ami: pop, xtcav, hsd, photfind and linearity, xpp
good ami: blobfinder, hitfinder, flyscan, wf sideband sub, stepscan, wf integration vs delay stage, normalization of andor, ratio of peak areas and epics, calculator, complex, timestamp plot, export, reference plot

phil recommends divide by 16adu/kev
mikhails formula (sig-ped)/gain to get kev
det.gain returns something to divide or multiply by?
- silke multiplies by det.gain for epix100
epix10k 10 for high gain .01 for low gain
phil uploads adu/kev because mikhail divides
may be inconsistent with documentation
consistent handling (adu/kev or kev/adu) of:
- uploaded numbers
- file on disk deployed by makepeds
- value by det.gain
- documentation
epix100, rayonix for lcls1.  try to be consistent in lcls2
gain goes into pixel_gain file
mikhail has 0.06kev/adu

other detectors (jungfrau, epix10k): adu/kev->divide
epix100: ev/adu (.06kev/adu)->multiply (lcls1, other way round for lcls2)
many files have relative gains (around 1)
propose lcls1 "gain-factor": kev/adu

mark hunter:
institutional/personal pressure (sebastien helped prioritize)
collaborative vs. competitive access to resources

**********************************************************************

ami-local -b 1 -f interval=1 psana://exp=ueddaq02,run=569,dir=/cds/data/psdm/prj/public01/xtc/

dan and seshu
scripts vs ami
people develop their own libraries (personal/hutch):
  commonly used pieces moved into core
three categories:
  using existing boxes
  creating custom boxes
  detector guts
mask gain bits
thresholded image
managing libraries (avoiding creeping featurism)
editing library files with text editor
accessing calibconst: raw-pedestal (psana/psana/detector/UtilsEpix10ka.py
customizing epix calibration
controlling kwargs, e.g. cmpars
https://confluence.slac.stanford.edu/display/PSDMInternal/Detector+Interface+Proposal

**********************************************************************

rix high-rate risks:

moving DAQ to SRCF:
ric claus work
- high rates: hsd, wave8, piranha, bld
- selecting events in ami
matt: high rate bld
switching to LCLS2 timing
dan archon
s3df 

risks:
complexity of running detectors at different rates
high-rate mono-encoder (two?), high rate bld
using sequencer so andor can cleanly integrate over shots
new dets: epixM, high rate mono, archon, bld
practicing high-rate rix analysis in advance
managing which events AMI sees
s3df delays
IOC detectors (like andor) need new module from Kukhee

**********************************************************************

tmo detectors:

2 piranhas (fzp, atm) + 1 opal (fzp) + 1 opal (cvmi)
1 for dream atm + 1 spare
currently have 4 (2 cvmi, 1 atm, 1 fzp)
bld

rix priority:

2 andor newton
3 wave8
1 atm piranha
1 atm opal
hsd's?
bld
high-rate mono encoder and/or step-scans

**********************************************************************

general to do:
- recabling to srcf
  o use data volume to each node to figure out cabling
  o (optional) multi-camlink operation.
    - could program timing link on reboot
    - every process could set full link mask?
  o what should go through the bos?
- switching to LCLS2 timing (Dec. 1?)
- 33kHz BLD: ebeam, gmd, test asap. waiting for software development.
- commission s3df (but keep old system going too)
  o psana
  o smalldata
  o movers
  o mon-node calibdir
- spares (piranha in particular)
- migrate ctl nodes (move hsd processes, but not mono encoder)
- move timing crate from fee alcove to 208
- xtcav "nice" for acr?
- live mode


tmo to-do:
- when can we switch from opals to piranhas?
- (optional) multi-camlink running
- will fzp/atm opals run at same time?
- hsd fex overflow damage flagged?
- test hsd prescale raw waveform buffering
- move away from xpm0 to xpm2 for all tmo timing

tmo schedule:

[cvmi_opal_1, oct 27, nov 19] - retire date is negotiable, (maybe needs to be earlier?)
[cvmi_opal_2, oct 27, nov 19] - retire date is negotiable, (maybe needs to be earlier?)
[atm_opal,       oct 27, possibly run 22]
[fzp_opal,        oct 27, possibly run 22]
[atm_pirhana, oct 27, not planned]
[fzp_pirhana,  oct 27, not planned]

rix to-do:
- integrating-andor timestamping (I think Dan said this is fine as-is)
- accelerator burst-mode operation
- scans with burst-mode (one burst is one andor event)
- (somewhat optional) high-rate interpolated mono encoder for fly-scans
  o offline second-pass with interpolation could also work
  o step-scans with slow mono are another option

**********************************************************************

100kHz data volumes:

- wave8: 8waveforms*256samples/sample*2bytes/sample*0.1MHz = 400MB/s
  need to add FEX data too
- piranha: 2048px*2bytes/px*0.1MHz=410MB/s
- hsd full wf: 60000samples*2bytes/sample*0.1MHz = 12GB/s
  max waveform length appears to be ~60000 samples looking at xtc2 file?
  FEX needs to reduce it to 4GB/s (3x)
- bld: 336bytes/event*0.1MHz = 33MB/s
  (from xtcreader -f /cds/data/psdm/prj/public01/xtc/tmoc00118-r0222-s008-c000.xtc2)
- timing: 309bytes/event*0.1MHz = 31MB/s

recabling proposal:
- tmo timing (including pvadet, bld)
- rix timing
- (4) tmo camlink nodes
- (2) rix camlink nodes
- tmo/rix fim node (5 fims)
- (2) rix hsd nodes
- (7) tmo hsd nodes
- peppex hsd node
- (2) tmo/rix ami
- (2) tmo/rix teb
- (2) tmo/rix mebuser

total of 25 nodes
5 spares

**********************************************************************

3.10 errors:

qt errors in groupca/xpmpva
control.py qt setvalue wants int but getting float
piranha rogue errors

**********************************************************************

rix analysis

point detector APD via wave8, background subtract
direct andor integrate over shots and background subtract
vls andor no background
might want to run at 1Hz with bigger images
filtering (using the signal itself like APD, or other det: I0 from fim's, GMD, use goose trigger or bykik (laser/xray on/off))
binning in terms of delay (vitara) eventually use timetool
fly-scan with mono-encoder and bin
after binning (vitara, eventually timetool, mono encoder) do normalization (I0 kb fim's)
no peak-finders
could run droplet on vls andor (droplets may merge) background low enough that not necessary?
could we run the andors in different modes?  might be complex in burst mode

march: beam at the mono
april: beam in chemrix

mockup piranha at 33kHz by modifying opal?

propose 33kHz analysis as "worst case" (gets easier with bursts and integrating detectors)

**********************************************************************

according to bos:
cmp11 goes to amc0 port 5 (checked)
cmp12 goes to amc0 port 3 (checked)
cmp06 goes to amc0 port 6 (trusting bos aliases)

**********************************************************************

worries:
political assigning time
off-shift handling?  preemption or queue position?
reservations more wasteful
preemption latency

1 - reservations + head-of-queue-qos
2 - ffb preemption qos + head-of-queue-off-shift qos
3 - 4 layers of preemption

association: priorities based on partition/account/reservation/qos (either for preemption or position-in-queue)
slurmacctmgr command is used to manage priorities

**********************************************************************

outer product options:

need two: one with hsd channel itself, and one with piranha readout

- only do half of the symmetric outer product (with hsd channel itself)
- reduce the sample rate by ~4
- reduce window to 2us (12000 samples) 1us @1MHz
- use peaks (guess badness is 5+-3)
- replace with tixel eventually (which does thresholding)
  o may not use tixel because of the thresholding, depending on expt
- use pre-scaled full-waveform events for bkgd?

**********************************************************************

epixhr

include forced-low, forced-medium
for AHL, AML
new format for timing (2 lanes each with 2 asics, each with timing info)

saci bus lockup?
same firmware on receiving end?

**********************************************************************

2 cfg 1a:a cmp017 dev1 in cnf but txlinkid says should be dev0
3 cfg 1a:b cmp020 dev0 in cnf but txlinkid says should be dev1 
5 cfg 3d:b cmp018 dev0 in cnf but txlinkid says should be dev1
6 cfg 3d:a cmp021 dev0 in cnf no pgp lock
8 5e:a cmp022 dev1 in cnf but txlinkid says should be dev0
9 5e:b cmp022 dev0 in cnf but txlinkid says should be dev1
10 da:b cmp024 dev0 in cnf but txlinkid says should be dev1
11 da:a cmp024 dev1 in cnf but txlinkid says should be dev0

tmo-drp[37848]: <I> MebCtrb   sent Enable @ 1034556513.272083936 (00011d3c6f8840) to MEB ID 1 @   0x7f7
7fc4ef000 (00080000 + 12 * 00800000)
tmo-drp[37848]: <C> PGPReader: Jump in complete l1Count 13459 -> 13444 | difference -15, tid L1Ac
cept
tmo-drp[37848]: <C> data: 3f99cfcd 0c00011d 1b87f332 3daa149a d3290001 00003484 service: 0xc
tmo-drp[37848]: <C> lastTid L1Accept
tmo-drp[37848]: <C> lastData: 3f99b1b6 0c00011d 1b07f32f 3daa149a cf270001 00003493
tmo-drp[37848]: <E> broken event:  00000000
tmo-drp[37848]: <E> broken event:  00000000
tmo-drp[37848]: <E> broken event:  00000000
tmo-drp[37848]: <E> broken event:  00000000
tmo-drp[37848]: <E> broken event:  00000000

ana: mona, mikhail, valmar, riccardo, ddamiani
daq: claus, matt, cpo, valmar, caf

**********************************************************************

fee running

early november (old timing)
late january (new timing)

slow power meter pv camera un-timestamped (phil heimann)
gmd/xgmd
ebeam bld?
epicsarch bunch of slow pv's

**********************************************************************

ansys:

- linux, not ubuntu, rhel ok
- mpi
- used lsf in past, need to figure out if slurm is ok?
- license already on an existing slac server (nodenames license1)
- ok to wait in queue
- want 512 cores (ran 128 cores a couple of years in the centos7 cluster)
- thermal loads on mono, xpp (diling), dxs (hasan) or spectrometer
- juhao would install, previously was a postdoc (gzhou)
- previously have used existing mpi installations (could try conda)
- also include:
  - slac's ansys point-of-contact: singularity?
  - ansys engineer

1DN=50e
0.2DN=10e

**********************************************************************

abaqus (amir shojaie)
https://drive.google.com/drive/folders/1EXyVkaVju9CAVzN_ZccaIT_taS9w1vzx
