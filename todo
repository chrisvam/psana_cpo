* shmem corrupt, andor, shmem corruption, xtcav, asserts, zeromq, ami xpcs, lv96 translation, power cycling, daq alarms (boss, fibers 208-to-tmo, procmgr)
* new-style psana1:

 PHASE1:
 - (switch default to new-style: change /reg/g/psdm/etc/psconda.sh to new-style)
   o py2 by default
   o py3 if users choose (add -py3 option to psconda.sh)
 - understand new mpi? (e.g. IB segfaults) but cheetah suggests maybe OK
 - do at same time that we move to new nfs ("/cds")
 - keep the old style available if users desperately need it (old /reg/g/psdm/etc/psconda.sh becomes /reg/g/psdm/etc/psconda_old.sh)
 - calibman working (dark runs most important)
 - advertise reduced features to users: no psana modules, no old-style translator, no mpi-h5 mpi-pytables
 - move old-style/new-style to /cds
 - jenkins release (and nightly?) into new conda area
 - need mpi hp5 for silke

 PHASE2:
 - (freezing old-style psana1-py2, and merging branches)
 - merge py3->master and tag
 - merge anarel-manage (removing py3 hacks)
 - make a new branch master->old_style_master (could work off this)
 - judgment call: how much to increase test coverage?  testing is currently minimal.

* xtcav recorder for mcc
* 20GB/s filesystem
* mikhail: roentdek with quadanode
* daq window placement
* dan: timehistory
* seshu: fix missing-data handling
* state machine confusing (ric email) lower priority?
* timetool? (lower priority)
* psana/ami survive "reset" (lower priority)
* official names for tmo
*cpo: shmem corruption
* prometheus python (eventually daq2 prometheus gui)
* monitor deadtime per-lane (matt)
* improved grafana (like ganglia)
* reduction algorithms (ric)
* move other hutches to lcls2
** lcls2 equivalent of xpptut15
* allocation of readout groups and devices, lanes
** Detector interface for filter_function and destination (mona)
* shared drp node allocation mechanism
* move to gcc9 (valerio)
* xpplr8116 geom calib const access for py3
* MHz with real tmo analysis (mona)
* protect against illegal python names in dotted-hierarchy (e.g. ClinkFeb[0])
* create more platforms for procmgr
** practice power outage
* debug shmem long-lived data corruption (pickN)
** detnames/detnames-minus-e within python
* parallel jupyterhub with visualization
* daq2 chunking
* lcls2 private calibdir (mikhail)
* encoder for rix
* portable gpu detector corrections (kokkos/hip/openmp/opencl?)
* deploy releases everywhere (nersc, sdf, new/old psana1)
* have psana automatically detect which detectors are requested and only use those streams (mona)
* why don't we get a c++ exception when we receive a configdb timeout?
* unified/integrated timetool calibration
* understand why we get corrupt pickN data if we don't copy l1accepts
* fix failing new-style psana1 tests (valerio)
* shifter mounting permissions (mona, johannes)
* problems on failure-modes confluence page
* disable IB in mpi until we understand intermittent crashes/warnings
* send multiple copy of events to shmem like lcls1
** small h5 ebeam/gasdet automatic storage
* small h5 ragged arrays
* test daq sequences (caf)
* psana1 MTRX:V2 geometry (mikhail)
* update procServ (caf)
* slow updates need to obey deadtime (matt)
* setting msgdelay in timing system? (matt)
* right now procmgr sets PATH to conda_rel.  could it "conda activate"?
* syslog print throttling
* off-by-one (psana1/2)
* units support in det xface
* put psana2 (and psana1?) on conda-forge
* remote visualization/control
* calibdb dns issue (in travis macos build)
* psana hutch/exp specific algs (mona, silke, dan, seshu)
* in psana SRV callback only persist some fields to h5?
* once we have real data, work more on timetool calc in firmware
* dlopen for reduction alg
* test calibman/geo (mikhail)
* nersc calib-dir sync (wilko)
* in-line documentation tools for lcls2
* psana1: continuous integration of py3 (jenkins)
** lcls2 configdb tools: history, copying
* algorithms (drp/ana, e.g. beam-center finding)
* psana startup slope, smd0 performance with small batch size, end-job poor behavior (mona)
* singularity at slac (mona)
* support more python versions
** SDF support including ARP
** python DRP? (matt)
* slurm and correcting problems with lsf
* slurm support in calibman (mikhail)
* consistent data format for opal/fakecam/xtcav.  Maybe not timetool since it's 1D.
* timetool: sometimes 1020, sometimes 1024 bytes (matt)
** test power outages (ric, onsite)
* fiber power readings everywhere (matt, tid)
* peaknet
* ami features: projection along curve, angular integration, epics import + looping, travis, xarray reading, mpi offline/evt-jumping, subgraphs, gui regression (run graphs), web xface, combine coarse+fine(timetool) timing, edge case: graph surgery global operation (worker/collector1and2) disconnect/reconnect, mask generation, josh requests, hsd time axis, hover to get docstrings, performance monitoring (prometheus)?, reopen displays on graph load, default size larger, default symbol, scatter plot number of points too large, hide "configure" menu running online, make "apply" more obvious/easy, reset kills ami (likely psana), plot vs readable time, hover over plot to get point (1D, 2D)
  - josh: free-form scan (no steps)
  - alex: set scan bin size limits (is histogram ok?)
  - josh: nanosecond xpcs: 2 pulse acqiris.  ratio of peak areas, correlation
    export results to ACR
  - josh: xpcs photon counting, working with chuck: accumulate statistics
    and then fit, number of photons per shot in histogram (talk to chuck
    and silke sxrm23). nicholas burdet (shared postdoc)
  - alex: manta camera (allied vision):  project, fit a peak (gaussian),
    plot vs. other variable,  ideally integrate over events, but
    difficult for ami. 30Hz.  run in mode with only a single shot per image.
* timetool (ben): tag to front end, fiber power, toggle xpmini->lcls2 timing, clear readout
* simpler interface for controlling teb/meb?
* xtcav daq recording epics variables as well (and also bld values?) (matt)
* lcls2 areadetector (valerio+mikhail)
** xtcav/psocake py3 compatible
